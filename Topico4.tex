\documentclass[../Notas.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

%\section{Tópico 4}

\section{Esperança de variáveis aleatórias discretas}
Começamos com uma motivação para depois introduzirmos a definição de esperança: dada uma amostra $\{ a_1, a_2, \dots, a_n \}\subset\mathbb{R}$, a média amostral é dada por $\displaystyle{ \frac{a_1 + a_2 + \cdots + a_n}{n} }$. Podemos representar esses dados numa tabela:
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        Valores distintos na amostra & Frequência absoluta \\
        \hline
        $x_1$ & $N_1$ \\
        $x_2$ & $N_2$ \\
        \vdots & \vdots \\
        $x_m$ & $N_m$
    \end{tabular}
%    \caption{Caption}
%    \label{tab:my_label}
\end{table}
Note que $N_i$ é o número de vezes que $x_i$ aparece na amostra, $i= 1,2,\dots, m$. Além disso, $N_1 + N_2 + \cdots + N_m = n$ e $x_1N_1 + x_2N_2 + \cdots + x_mN_m = a_1 + a_2 + \cdots + a_n$. Com isso, podemos escrever a média dos $a_i$ como $\displaystyle{ \frac{x_1N_1 + x_2N_2 + \cdots + x_mN_m}{N_1 + N_2 + \cdots + N_m} }$ ou, ainda, $\displaystyle{ x_1\frac{N_1}{n} + \cdots + x_m\frac{N_m}{n} }$, em que $N_i/n$ é a frequência relativa de $x_i$, $i=1,2,\dots,m$. Nesse caso, a f.p. $p:\mathbb{R}\to\mathbb{R}$ é dada por
\begin{align*}
    p(x) = \begin{cases}
    N_i/n, x=x_i \text{ para algum } i=1,2,\dots,m \\
    0, \text{ c.c.}
    \end{cases}.
\end{align*}
Agora, suponhamos que $x_1, x_2, \dots, x_m$ sejam valores possíveis de uma v.a. $X$ e que $a_1, a_2, \dots, a_n$ são valores (independentes) observados de $X$. Então, de acordo com a interpretação da probabilidade como frequência relativa (Lei Forte dos Grandes Números), temos que para $n$ grande a frequência relativa se aproxima da probabilidade real, i.e.,
\begin{align*}
    \lim_{n\to\infty}\frac{N_i}{n} = p(x_i) = P(X=x_i).
\end{align*}
Assim, o \textbf{valor esperado} de $X$, representado por $EX$ ou $E[X]$, é $\displaystyle{ EX = \sum_{i=1}^{m}x_i p(x_i) }$, sendo $p$ a f.p. de $X$.

\begin{definition}[Esperança]
Seja $X$ uma v.a. discreta em $(\Omega, \mathcal{A}, P)$ com valores possíveis $\{ x_1, x_2, \dots \}$. Se $\displaystyle{ \sum_{i} |x_i|p(x_i) < \infty }$, dizemos que $X$ tem \textbf{esperança finita} e definimos sua esperança ou \textbf{esperança matemática} ou \textbf{valor esperado} ou \textbf{média} como
\begin{align*}
    EX = \sum_{i=1}^{\infty}x_ip(x_i) = \sum_x xp(x).
\end{align*}
Note, em particular, que se o conjunto dos valores possíveis de $X$ é finito, então $X$ tem esperança finita.
\end{definition}

\begin{example}
Seja $X\sim B(n,p)$. Temos
\begin{align*}
    p_X(k) = \begin{cases}
    \binom{n}{k}p^k(1-p)^{n-k}, k = 0,1,\dots,n \\
    0, \text{ c.c.}
    \end{cases}.
\end{align*}
Como $X$ tem uma quantidade finita de valores possíveis, então $X$ tem esperança finita dada por
\begin{align*}
    EX = \sum_x xp(x) = \sum_{k=0}^{n} kP(X=k) = \sum_{k=1}^{n} k\binom{n}{k}p^k(1-p)^{n-k} &= \sum_{k=1}^{n} \frac{n!}{(k-1)!(n-k)!}p^k(1-p)^{n-k} \\
    &= np\sum_{k=1}^{n} \binom{n-1}{k-1}p^{k-1}(1-p)^{n-1-(k-1)} \\
    &= np.
\end{align*}
Em particular, se $n=1$ então $EX = p$.
\end{example}

\begin{example}
Seja $X\sim\text{Poisson}(\lambda)$. Temos
\begin{align*}
    p_X(k) = \begin{cases}
    e^{-\lambda}\frac{\lambda^k}{k!}, k = 0,1,\dots \\
    0, \text{ c.c.}
    \end{cases}.
\end{align*}
Note que como $X$ tem infinitos valores possíveis, não sabemos a priori se $EX < \infty$. Temos
\begin{align*}
    \sum_x |x|p_X(x) = \sum_{k=0}^{\infty} |k|e^{-\lambda}\frac{\lambda^k}{k!} = \lambda e^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!} = \lambda < \infty.
\end{align*}
Logo, $X$ tem esperança finita e, como seus valores possíveis são não negativos, segue que $EX=\lambda$.
\end{example}

\begin{example}
Seja $X\sim\text{Geom}(p)$. Temos
\begin{align*}
    p_X(k) = \begin{cases}
    p(1-p)^{k-1}, k = 1,2,\dots \\
    0, \text{ c.c.}
    \end{cases}, 0 < p < 1.
\end{align*}
Note que
\begin{align*}
    \sum_x |x|p_X(x) = \sum_{k=1}^{\infty} |k|p(1-p)^{k-1} = p\sum_{k=1}^{\infty} k(1-p)^{k-1} &= -p\sum_{k=1}^{\infty} \dfrac{d}{dp}[(1-p)^k] \\
    &= -p\dfrac{d}{dp}\left[ \sum_{k=1}^{\infty} (1-p)^k \right] \\
    &= -p(-1/p^2) \\
    &= 1/p,
\end{align*}
em que a série converge pelo teste da integral. Logo, $X$ tem esperança finita e, como seus valores possíveis são não negativos, temos $EX=1/p$.
\end{example}

\begin{example}
Considere $p:\mathbb{R}\to\mathbb{R}$ dada por
\begin{align*}
    p(x) = \begin{cases}
    \frac{1}{x(x+1)}, x = 1,2,\dots \\
    0, \text{ c.c.}
    \end{cases}
\end{align*}
Note que $p(x)\geq 0, \forall x\in\mathbb{R}$, $\{x\in\mathbb{R} : p(x) > 0\} = \mathbb{N}$ é enumerável e $\displaystyle{ \sum_x p(x) = \sum_{x=1}^{\infty}\frac{1}{x(x+1)} = \lim_{N\to\infty}\sum_{x=1}^N \frac{1}{x} - \frac{1}{x+1} = \lim_{N\to\infty} 1 - \frac{1}{N+1} = 1 }$. Logo, $p$ é uma f.p. e, se $X$ é v.a. com f.p. $p$, temos
\begin{align*}
    \sum_x |x|p(x) = \sum_{x=1}^{\infty}\frac{1}{x+1}
\end{align*}
que diverge pelo teste da integral. Logo, $X$ não tem esperança finita e escrevemos $EX = +\infty$.
\end{example}

\subsection{Esperança de função de variável aleatória}
Nosso interesse aqui é determinar a esperança de uma v.a. (discreta) que é função de uma ou mais v.a.'s: $Z = g(X)$, com $X = (X_1, \dots, X_n)$ e $g:\mathbb{R}^n\to\mathbb{R}$ função. Note que o conjunto de valores possíveis de $X$ é $\{x_1, x_2, \dots \}\subset\mathbb{R}^n$ (enumerável), ou seja, $P(X=x_j) > 0, j = 1,2,\dots$ e $P(X\in\{x_1, x_2, \dots\}) = 1$. Note que $x_j = (x_{j_1}, \dots, x_{j_n})\in\mathbb{R}^n, j=1,2,\dots$ e $\displaystyle{ \sum_x g(x)p_X(x) = \sum_j g(x_j)p_X(x_j) }$.

\begin{theorem}
Sejam $X = (X_1, \dots, X_n)$ um vetor aleatório discreto com f.p. $p_X$ e $g:\mathbb{R}^n\to\mathbb{R}$ função. A v.a. $Z = g(X)$ tem esperança finita se, e só se, $\displaystyle{ \sum_x |g(x)|p_X(x) < \infty }$. Nesse caso,
\begin{align*}
    EZ = \sum_x g(x)p_X(x) = \sum_{x_1}\cdots\sum_{x_n} g(x_1, \dots, x_n)P(X_1 = x_1, \dots, X_n=x_n).
\end{align*}
\end{theorem}

\begin{proof}
Vamos mostrar o caso $n=1$; o caso $n>1$ é análogo. Temos $g:\mathbb{R}\to\mathbb{R}$ função e, se $\{x_1, x_2, \dots\}$ é o conjunto de valores possíveis de $X$, então $\{ g(x_1), g(x_2), \dots, g(x_n) \}$ é o conjunto de valores possíveis de $Z = g(X)$. Note que pode haver repetição, i.e., podemos ter $g(x_i) = g(x_k)$ com $x_i\neq x_k$. Seja $A_j = \{ x : x=x_i \text{ e } g(x) = z_j \}, j = 1,2,\dots$, sendo $\{z_1, z_2, \dots\}$ o conjunto de valores possíveis de $Z$. Note que $\{X\in A_j\} = \{Z=z_j\}$ e, para cada $j=1,2,\dots$, $A_j\subset\{ x_1, x_2, \dots \}$ e os $A_j$'s são dois a dois disjuntos já que $g$ função. Além disso,
\begin{align*}
    \bigcup_{j=1}^{\infty}A_j = \{x_1, x_2, \dots\}.
\end{align*}
Logo, 
\begin{align*}
\sum_j |z_j|p_Z(z_j) = \sum_j |z_j|P(X\in A_j) &= \sum_j |z_j|\sum_{i : x_i\in A_j} P(X=x_i) \\
&= \sum_j\sum_{i:x_i\in A_j} |z_j|P(X=x_i) \\
&= \sum_j\sum_{i:x_i\in A_j}|g(x_i)|P(X=x_i) \\
&= \sum_k |g(x_k)|P(X=x_k).
\end{align*}
Logo, $EZ < \infty \iff \displaystyle{ \sum_x |g(x)|p_X(x) < \infty }$. Agora, se $EZ < \infty$, então analogamente ao acima, $EZ = \displaystyle{ \sum_x g(x)p_X(x) }$.
\end{proof}

\begin{remark}
Dada uma v.a. discreta $X$, vimos que (i) $EX < \infty \iff \displaystyle{ \sum_x |x|P(X=x) < \infty }$ (por definição) e (ii) $E|X|$, quando existe, é $\displaystyle{ \sum_x |x|P(X=x)}$ pelo teorema acima. Logo, $EX < \infty \iff E|X| < \infty$.
\end{remark}

\begin{example}
Sejam $X, Y$ v.a.'s independentes com $X\sim\text{Poisson}(\lambda_1)$ e $Y\sim\text{Poisson}(\lambda_2)$. Vamos calcular $E[X^2], E[XY]$ e $E[X+Y]$.
\begin{enumerate}[(i)]
    \item Pelo teorema,
    \begin{align*}
        \sum_x |g(x)|p_X(x) &= \sum_{k=0}^{\infty} k^2e^{-\lambda_1}\frac{\lambda_1^k}{k!} \\
        &= \sum_{k=1}^{\infty} ke^{-\lambda_1}\frac{\lambda_1^{k}}{(k-1)!} \\
        &= \lambda_1\left[ \sum_{k=1}^{\infty}(k-1)e^{-\lambda_1}\frac{\lambda_1^{k-1}}{(k-1)!} + \sum_{k=1}^{\infty}e^{-\lambda_1}\frac{\lambda_1^{k-1}}{(k-1)!} \right] \\
        &= \lambda_1[EX + e^{\lambda_1}e^{-\lambda_1}] \\
        &= \lambda_1(\lambda_1 + 1)\in\mathbb{R}.
    \end{align*}
    Logo, $X^2$ tem esperança finita e, como seus valores possíveis são não negativos, $E[X^2] = \lambda_1(\lambda_1 + 1)$.
    
    \item Pelo teorema,
    \begin{align*}
        \sum_x\sum_y |g(x,y)|p_{X,Y}(x,y) &= \sum_{k=0}^{\infty}\sum_{j=0}^{\infty}kjP(X=k, Y=j) \\
        &= \sum_{k=1}^{\infty}\sum_{j=1}^{\infty} kje^{-\lambda_1}\frac{\lambda_1^k}{k!}e^{-\lambda_2}\frac{\lambda_2^j}{j!} \\
        &= \sum_{k=1}^{\infty}\left[ ke^{-\lambda_1}\frac{\lambda_1^k}{k!}\sum_{j=1}^{\infty}je^{-\lambda_2}\frac{\lambda_2^j}{j!} \right] \\
        &= \lambda_2\sum_{k=1}^{\infty} ke^{-\lambda_1}\frac{\lambda_1^k}{k!} \\
        &= \lambda_1\lambda_2\in\mathbb{R},
    \end{align*}
    logo $XY$ tem esperança finita e $E[XY] = \lambda_1\lambda_2$ pois os valores possíveis de $XY$ são não negativos.
    
    \item Temos, pelo teorema, que
    \begin{align*}
        \sum_x\sum_y |g(x,y)|p_{X,Y}(x,y) &= \sum_{k=0}^{\infty}\sum_{j=0}^{\infty} (k+j)P(X=k, Y=j) \\
        &= \sum_{k=0}^{\infty}k\sum_{j=0}^{\infty} P(X=k, Y=j) + \sum_{j=0}^{\infty}j\sum_{k=0}^{\infty} jP(X=k, Y=j) \\
        &= \sum_{k=0}^{\infty}kp_X(k) + \sum_{j=0}^{\infty} jp_Y(j) \\
        &= EX + EY \\
        &= \lambda_1 + \lambda_2\in\mathbb{R},
    \end{align*}
    logo $X+Y$ tem esperança finita e $E[X+Y] = EX + EY = \lambda_1 + \lambda_2$ pois os valores possíveis de $X+Y$ são não negativos.
\end{enumerate}
\end{example}

\begin{theorem}[Propriedades da esperança]
Sejam $X$ e $Y$ v.a.'s definidas em $(\Omega, \mathcal{A}, P)$ com $EX, EY < \infty$. Temos
\begin{enumerate}[(a)]
    \item $P(X=c) = 1\implies EX = c, \forall c\in\mathbb{R}$
    \item $E[cX] < \infty$ e $E[cX] = cEX, \forall c\in\mathbb{R}$
    \item $E[X+Y] < \infty$ e $E[X+Y] = EX+EY$. De maneira geral, dados $c_1, \dots, c_n\in\mathbb{R}$ e $EX_1, \dots, EX_n < \infty$, então
    \begin{align*}
        E[c_1X_1 + \cdots + c_nX_n] = \sum_i c_iEX_i < \infty
    \end{align*}
    \item Se $P(X\geq Y) = 1$, então $EX\geq EY$ e $EX = EY\iff P(X=Y) = 1$. Em particular, $P(X\geq 0) = 1 \implies EX \geq 0$
    \item $|EX|\leq E|X|$.
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}[(a)]
    \item Temos $\displaystyle{ \sum_x |x|p(x) = |c| < \infty } \therefore EX = c < \infty$.
    \item Temos $\displaystyle{ \sum_x |cx|p(x) = |c|\sum_x |x|p(x) < \infty \therefore E[cX] = cEX < \infty }$.
    \item Temos
    \begin{align*}
        &\sum_{x_1}\cdots\sum_{x_n}|c_1x_1 + \cdots + c_nx_n|P(X_1=x_1, \dots, X_n=x_n) \\
        &\leq \sum_{x_1}|c_1x_1|\sum_{x_2}\cdots\sum_{x_n}P(X_1=x_1, \dots, X_n=x_n) + \cdots + \sum_{x_n}|c_nx_n|\sum_{x_1}\cdots\sum_{x_{n-1}}P(X_1=x_1, \dots, X_n=x_n) \\
        &= |c_1|\sum_{x_1}|x_1|P(X_1=x_1) + \cdots + |c_n|\sum_{x_n}|x_n|P(X_n=x_n) < \infty.
    \end{align*}
    Logo, $\displaystyle{ E\left[ \sum_{i}c_iX_i \right] = \sum_i c_iEX_i < \infty .}$
    
    \item Se $P(X\geq Y) = 1$, então $x_i\geq y_j$ para todo par de valores possíveis $(x_i, y_j)$. Daí,
    \begin{align*}
        EX = \sum_x xP(X=x) \geq \sum_y yP(X=x) \geq \sum_y yP(Y=y) = EY.
    \end{align*}
    Se $P(X=Y) = 1$, então $X$ e $Y$ têm o mesmo conjunto de valores possíveis e $P(X=x) = P(Y=y)$. Logo, $EX = \sum_x xP(X=x) = \sum_y yP(Y=y) = EY$. Se $EX=EY$, devemos ter $X$ e $Y$ com o mesmo conjunto de valores possíveis e $P(X=x) = P(Y=y)$.
    
    \item $\displaystyle{|EX| = \left| \sum_x xp(x) \right| \leq \sum_x |x|p(x) = E|X|}$.
\end{enumerate}
\end{proof}

\begin{theorem}
Seja $X$ v.a. definida em $(\Omega, \mathcal{A}, P)$ tal que $P(|X|\leq M) = 1, M\in\mathbb{R}$. Então $EX < \infty$ e $|EX|\leq M$.
\end{theorem}

\begin{proof}
Seja $x$ valor possível de $X$. Se $|x|>M$, então $P(|X|>M)\geq P(X=x)>0$. Mas $P(|X|>M) = 1 - P(|X|\leq M) = 0$. Logo, $|x|\leq M$ e, assim,
\begin{align*}
    \sum_x |x|p_X(x) \leq M\sum_x p_X(x) = M < \infty,
\end{align*}
ou seja, $EX < \infty$. Por fim,
\begin{align*}
    |EX| \leq E|X| \leq EM = M.
\end{align*}
\end{proof}

\begin{example}
Se $X\sim B(n,p)$, então $EX = np$, como vimos. Outra maneira de calcular $EX$ seria tomar $X_i\sim B(1,p), i=1,\dots, n$ e $S_n = X_1 + \cdots + X_n\sim B(n,p)$. Então $ES_n = E[X_1 + \cdots + X_n] = \sum_i EX_i = np$.
\end{example}

\begin{example}
Seja $X\sim\text{Hgeo}(N,m,n)$. Temos
\begin{align*}
    p_{S_n}(k) = \begin{cases}
    \frac{ \binom{m}{k}\binom{N-m}{n-k} }{ \binom{N}{n} }, k = 0,1,\dots,n \\
    0, \text{ c.c.}
    \end{cases}.
\end{align*}
Poderíamos calcular $ES_n$ pela definição, mas isso seria trabalhoso e complicado; vamos proceder de outro modo. Sejam $X_i$ v.a.'s indicadoras da presença do objeto de tipo $i$. Temos, então,
\begin{align*}
    EX_i = \frac{ \binom{1}{1}\binom{N-1}{n-1} }{\binom{N}{n}} = \frac{n}{N}, i = 1, 2, \dots, n.
\end{align*}
Logo, $ES_n = \sum_i EX_i = mn/N$.
\end{example}

\begin{remark}
Vimos que $EX, EY < \infty$ implica $E[X+Y] = EX+EY$. Contudo, o mesmo não vale para $E[XY]$. De fato, sejam $X,Y$ v.a.'s com $p_X(-1) = 1/2 = p_X(1)$ e $Y=X$. A f.p. conjunta de $X$ e $Y$ pode ser representada como
\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c}
        $X\setminus Y$ & -1 & 1 & $p_X(x)$ \\
        \hline
        -1 & 1/2 & 0 & 1/2 \\
        1 & 0 & 1/2 & 1/2 \\
        \hline
        $p_Y(y)$ & 1/2 & 1/2 & 1
    \end{tabular}
%    \caption{Caption}
%    \label{tab:my_label}
\end{table}
Temos $EX = -1/2 + 1/2 = 0 = EY$ mas $E[XY] = 1/2 + 1/2 = 1 \neq EXEY$. É importante notar que $X$ e $Y$ \textbf{não são independentes!}
\end{remark}

\begin{theorem}
Sejam $X$ e $Y$ v.a.'s definidas em $(\Omega, \mathcal{A}, P)$ com $EX, EY <\infty$. Se $X$ e $Y$ são independentes, então $E[XY] < \infty$ e $E[XY] = EXEY$.
\end{theorem}

\begin{proof}
Temos
\begin{align*}
    \sum_x\sum_y |xy|p_{X,Y}(x,y) = \sum_x |x|p_X(x)\sum_y |y|p_Y(y) < \infty
\end{align*}
pois $EX, EY < \infty$. Daí, segue que
\begin{align*}
    E[XY] = \left( \sum_x xp_X(x) \right)\left( \sum_y yp_Y(y) \right) = EXEY.
\end{align*}
\end{proof}

\begin{remark}
Note que a recíproca deste teorema \textbf{é falsa!} O exemplo anterior é um contraexemplo disso.
\end{remark}

\begin{theorem}
Seja $X$ v.a. inteira não negativa, i.e., com conjunto de valores possíveis $\{0, 1, 2, \dots\}$. Então $EX < \infty \iff \displaystyle{ \sum_{x=1}^{\infty} P(X\geq x) < \infty }$ e, nesse caso, $EX = \displaystyle{ \sum_{x=1}^{\infty} P(X\geq x) = \sum_{x=0}^{\infty} 1 - F_X(x) }$. 
\end{theorem}

\begin{proof}
Temos que $EX < \infty$ se, e só se,
\begin{align*}
    \sum_x |x|p_X(x) = \sum_{x=1}^{\infty} xp_X(x) = \sum_{x=1}^{\infty}p_X(x) + \sum_{x=2}^{\infty}p_X(x) + \cdots = \sum_{k=1}^{\infty}\sum_{x=k}^{\infty}p_X(x) = \sum_{k=1}^{\infty}P(X\geq k) < \infty,
\end{align*}
em que a segunda igualdade vale pois a convergência é absoluta. Daí, quando existe, $EX$ é dada por
\begin{align*}
    \sum_{k=1}^{\infty} P(X\geq k) = \sum_{k=0}^{\infty} 1 - F_X(k).
\end{align*}
\end{proof}

\begin{example}
$X\sim\text{Geo}(p)$ é inteira não negativa, logo
\begin{align*}
    EX = \sum_{k=1}^{\infty} P(X\geq k) = \sum_{k=0}^{\infty} 1 - F_X(k) = \sum_{k=0}^{\infty} (1-p)^k = 1/p.
\end{align*}
\end{example}

\begin{example}
Sejam $X, Y\sim\text{Geom}(p)$ e $W = \min\{X,Y\}$. Temos $W$ inteira não negativa e
\begin{align*}
    F_W(w) = \begin{cases}
    0, w < 1 \\
    1 - (1-p)^{2[w]}, w\geq 1
    \end{cases}.
\end{align*}
Logo, 
\begin{align*}
    EW = \sum_{k=0}^{\infty} 1 - F_W(k) = \sum_{k=0}^{\infty} (1-p)^{2k} = \frac{1}{1-(1-p)^2} = \frac{1}{2p-p^2} = \frac{1}{p(2-p)}.
\end{align*}
\end{example}

\subsection{Momentos}
\begin{definition}[Momentos e variância]
Sejam $X$ v.a. e $k\in\mathbb{N}$. Se $E[X^k]<\infty$, dizemos que $X$ tem \textbf{momento de ordem} $k$ e chamamos $E[X^k]$ de \textbf{momento de ordem} $k$ ou $k$-ésimo momento de $X$. Quando $X$ tem momento de ordem $k$, chamamos $E[(X-b)^k]$ de \textbf{momento de ordem} $k$ em torno de $b, \forall b\in\mathbb{R}$. Em particular, se $k=2$ e $b=EX$, temos
\begin{align*}
    E[(X-EX)^2] := \Var(X) = \sigma_X^2 \ \text{ e } \ \sqrt{\Var(X)} := \sigma_X
\end{align*}
a \textbf{variância} e o \textbf{desvio-padrão} de $X$, respectivamente. É comum denotar também $EX = \mu$.
\end{definition}

\begin{proposition}[Propriedades dos momentos]
Os momentos de uma v.a. satisfazem as seguintes propriedades.
\begin{itemize}
    \item[(P1)] Se $X$ é v.a. com momento de ordem $r$, então $X$ tem momento de ordem $k$ para todo $k\leq r$.
    \item[(P2)] Se $X$ e $Y$ são v.a.'s com momento de ordem $r$, então $X+Y$ também tem momento de ordem $r$ e, de maneira geral, se $X_1, \dots, X_n$ são v.a.'s com momento de ordem $r$, então $X_1+\cdots+X_n$ também tem momento de ordem $r$ e, se $E[X^2] < \infty$, então $\Var(X) < \infty$.
    \item[(P3)] Seja $X$ v.a. com segundo momento finito. Então
    \begin{enumerate}[(i)]
        \item $\Var(X) \geq 0$
        \item $\Var(X) = E[X^2] - EX^2$
        \item $P(X=c) = 1 \iff \Var(X) = 0$
        \item $\Var(X+c) = \Var(X), \forall c\in\mathbb{R}$
        \item $\Var(cX) = c^2\Var(X), \forall c\in\mathbb{R}$
        \item Se $Y$ é v.a. com segundo momento finito, então $\Var(X+Y) = \Var(X) + \Var(Y) + 2E[(X-EX)(Y-EY)] < \infty$.
    \end{enumerate}
\end{itemize}
\end{proposition}

\begin{proof}
\begin{itemize}
    \item[(P1)] Da definição de esperança, temos $E[X^r] = \displaystyle{ \sum_x x^rp_X(x) < \infty }$ e essa série converge absolutamente. Logo, podemos derivá-la, obtendo os demais momentos.
    
    \item[(P2)] Note que $E[Y^r] < \infty \implies E[(-Y)^r] = (-1)^rE[Y^r] <\infty$, logo $-Y$ tem momento de ordem $k$. Note que
    \begin{align*}
        E[X^2] < \infty \implies EX < \infty \implies E[(EX)^2] = EX^2,
    \end{align*}
    logo $EX$ tem segundo momento finito; como $X$ e $EX$ têm segundo momento finito, então $X-EX$ também tem e $\Var(X) < \infty$.
    
    \item[(P3)]\begin{enumerate}[(i)]
        \item Temos
        \begin{align*}
            \Var(X) = E[(X-EX)^2] \geq 0
        \end{align*}
        pois $(X-EX)^2$ é não negativa.
        
        \item Temos
        \begin{align*}
            \Var(X) = E[(X-EX)^2] &= E[X^2 - 2XEX + EX^2] \\
            &= E[X^2] - 2EXEX + EX^2 \\
            &= E[X^2] - EX^2.
        \end{align*}
        
        \item Note que
        \begin{align*}
            P(X=c) = 1 \implies P(X^2=c^2) = 1 \implies E[X^2] = c^2 \therefore \Var(X) = c^2-c^2 = 0.
        \end{align*}
        Reciprocamente,
        \begin{align*}
            \Var(X) = 0 \implies E[(X-EX)^2] = 0 &\implies \sum_i \underbrace{(x_i - EX)^2}_{\geq 0} \underbrace{P(X=x_i)}_{> 0} = 0 \\
            &\implies x_i = EX, i=1,2,\dots \\
            &\implies EX = c \text{ é o único valor possível de } X \\
            &\therefore P(X=c) = 1.
        \end{align*}
        
        \item Temos
        \begin{align*}
            \Var(X+c) = E[(X+c - E[X+c])^2] &= E[(X + c - EX - c)^2] \\
            &= E[(X-EX)^2] \\
            &= \Var(X), \forall c\in\mathbb{R}.
        \end{align*}
        
        \item Temos
        \begin{align*}
            \Var(cX) = E[(cX - E[cX])^2] = E[c^2(X-EX)^2] = c^2\Var(X), \forall c\in\mathbb{R}.
        \end{align*}
        
        \item Temos
        \begin{align*}
            \Var(X+Y) &= E[(X+Y - E[X+Y])^2] \\
            &= E[(X+Y-EX-EY)^2] \\
            &= E[(X-EX)^2 + (Y-EY)^2 + 2(X-EX)(Y-EY)] \\
            &= \Var(X) + \Var(Y) +2E[(X-EX)(Y-EY)].
        \end{align*}
    \end{enumerate}
\end{itemize}
\end{proof}

\begin{example}
Seja $X\sim B(n,p)$. Temos
\begin{align*}
    E[X^2] &= \sum_{k=0}^{n}k^2\binom{n}{k}p^k(1-p)^{n-k} \\
    &= \sum_{k=1}^{n} [k(k-1) + k]\binom{n}{k}p^k(1-p)^{n-k} \\
    &= \sum_{k=1}^{n} k(k-1)\binom{n}{k}p^k(1-p)^{n-k} + \sum_{k=1}^{n} k\binom{n}{k}p^k(1-p)^{n-k} \\
    &= np + n(n-1)p^2\sum_{k=2}^{n}\binom{n-2}{k-2}p^{k-2}(1-p)^{n-2 - (k-2)} \\
    &= np + n(n-1)p^2 \\
    &= np(1-p) + n^2p^2 \therefore \Var(X) = n^2p^2 + np(1-p) - n^2p^2 = np(1-p).
\end{align*}
\end{example}

\begin{example}
Seja $X\sim\text{Geom}(p)$. Temos
\begin{align*}
    E[X^2] = \sum_{k=1}^{\infty} k^2 p(1-p)^{k-1} &= p\sum_{k=1}^{\infty} [(k-1)+1]^2 (1-p)^{k-1} \\
    &= \sum_{k=1}^{\infty} (k-1)^2 p(1-p)^{k-1} + \sum_{k=1}^{\infty} 2(k-1)p(1-p)^{k-1} + \sum_{k=1}^{\infty} p(1-p)^{k-1} \\
    &= (1-p)E[X^2] + 2(1-p)EX + 1.
\end{align*}
Daí,
\begin{align*}
    E[X^2] = \frac{2-p}{p^2} \therefore \Var(X) = \frac{2-p}{p^2} - \frac{1}{p^2} = \frac{1-p}{p^2}.
\end{align*}
\end{example}

\begin{definition}[Covariância]
Sejam $X,Y$ v.a.'s em $(\Omega, \mathcal{A}, P)$ com $E[X^2], E[Y^2] < \infty$. A \textbf{covariância} de $X$ e $Y$ é definida por
\begin{align*}
    \Cov(X,Y) = E[(X-EX)(Y-EY)] = \sigma_{X,Y}.
\end{align*}
\end{definition}

\begin{proposition}[Propriedades da covariância]
Sejam $X, Y, X_1, \dots, X_n, Y_1, \dots, Y_m$ v.a.'s com segundo momento finito. Temos
\begin{enumerate}[(i)]
    \item $\Cov(X,Y) = E[XY] - EXEY$
    \item $\Cov(X,Y) = \Cov(Y,X)$
    \item $\Cov(X,X) = \Var(X)$
    \item $\Cov(cX,Y) = c\Cov(X,Y) = \Cov(X,cY), \forall c\in\mathbb{R}$
    \item $\displaystyle{ \Cov\left( \sum_{i=1}^{n}X_i, \sum_{j=1}^{m}Y_j \right) = \sum_{i=1}^{n}\sum_{j=1}^{m} \Cov(X_i, Y_j) }$
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[(i)]
    \item Temos
    \begin{align*}
        \Cov(X,Y) = E[(X-EX)(Y-EY)] = E[XY - XEY - YEX + EXEY] = E[XY] - EXEY.
    \end{align*}
    \item Note que
    \begin{align*}
        \Cov(X,Y) = E[XY] - EXEY = E[YX] - EYEX = \Cov(Y,X).
    \end{align*}
    \item Temos
    \begin{align*}
        \Cov(X,X) = E[X^2] - EX^2 = \Var(X).
    \end{align*}
    \item Basta notar que
    \begin{align*}
        \Cov(cX, Y) = E[cXY] - E[cX]EY = c\Cov(X,Y) = \Cov(X,cY).
    \end{align*}
    \item Por fim, temos
    \begin{align*}
        \Cov\left( \sum_{i=1}^{n}X_i, \sum_{j=1}^{m}Y_j \right) &= E\left[ \sum_{i=1}^{n}\sum_{j=1}^{m} X_iY_j \right] - E\left[ \sum_{i=1}^{n}X_i \right]E\left[ \sum_{j=1}^{m}Y_j \right] \\
        &= \sum_{i=1}^{n}\sum_{j=1}^{m} E[X_iY_j] - EX_iEY_j \\
        &= \sum_{i=1}^{n}\sum_{j=1}^{m} \Cov(X_i, Y_j).
    \end{align*}
\end{enumerate}
\end{proof}

\begin{proposition}
Sejam $X_1, \dots, X_n$ v.a.'s com esperança finita. Então
\begin{align*}
    \Var\left( \sum_{i=1}^{n}X_i \right) = \sum_{i=1}^{n}\Var(X_i) + 2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\Cov(X_i, X_j).
\end{align*}
\end{proposition}

\begin{proof}
Usando a proposição acima, temos
\begin{align*}
    \Var\left( \sum_{i=1}^{n}X_i \right) = \Cov\left( \sum_{i=1}^{n}X_i, \sum_{j=1}^{n}X_j \right) 
    = \sum_{i=1}^n\sum_{j=1}^n \Cov(X_i, X_j) 
    = \sum_{i=1}^n \Var(X_i) + 2\sum_{i=1}^{n-1}\sum_{j=i+1}^n\Cov(X_i, X_j).
\end{align*}
\end{proof}

\begin{remark}
Note que se $X$ e $Y$ são independentes, então $\Cov(X,Y) = E[XY] - EXEY = 0$ e, se $X_1, \dots, X_n$ são independentes, então $\displaystyle{ \Var\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n\Var(X_i)}$.
\end{remark}

\begin{definition}[Correlação]
Se $\Cov(X,Y) = 0$, então $X$ e $Y$ são ditas \textbf{não-correlacionadas}.
\end{definition}

Note, em particular, que se $X$ e $Y$ são \textbf{não-correlacionadas}, então $\Var(X+Y) = \Var(X) + \Var(Y)$. Ademais, se $X$ e $Y$ são \textbf{independentes} então elas são não-correlacionadas, mas a recíproca é \textbf{falsa!} Intuitivamente, $X$ e $Y$ serem independentes significa que não existe \textbf{nenhuma relação} entre as duas v.a.'s; por outro lado, como veremos mais à frente, as duas v.a.'s serem não-correlacionadas significa que não existe \textbf{relação linear} entre ambas.

\begin{example}
Seja $S_n\sim\text{Hgeo}(N,m,n)$. Vimos que $S_n$ é a soma das variáveis indicadoras $X_i$, com $EX_i = n/N$ e $ES_n = mn/N$. Como $X_i^2=X_i$, então $E[X_i^2] = n/N$ e $\Var(X_i) = n/N(1 - n/N)$. Ademais, dados $1\leq i < j\leq m$, então
\begin{align*}
    E[X_iX_j] = P(X_i = 1, X_j = 1) = \frac{n}{N}\cdot\frac{n-1}{N-1}.
\end{align*}
Logo,
\begin{align*}
    \Cov(X_i, X_j) = \frac{n}{N}\cdot\frac{n-1}{N-1} - \frac{n^2}{N^2} = \frac{n}{N}\cdot\frac{n-N}{N(N-1)}
\end{align*}
e, daí,
\begin{align*}
    \Var(S_n) &= \sum_{i=1}^m \Var(X_i) + 2\sum_{i=1}^{m-1}\sum_{j=i+1}^m \Cov(X_i, X_j) \\
    &= \frac{mn}{N}\left(1 - \frac{n}{N}\right) + 2\sum_{i=1}^{m-1}(m-i)\frac{n}{N}\cdot\frac{n-N}{N(N-1)} \\
    &= \frac{mn}{N}\left(1 - \frac{n}{N}\right) + 2\left[ \frac{mn}{N}\cdot\frac{n-N}{N(N-1)}(m-1) + \frac{n}{N}\cdot\frac{n-N}{N(N-1)}\cdot\frac{m(m-1)}{2} \right] \\
    &= \frac{mn}{N}\cdot\frac{(N-n)(N-m)}{N(N-1)}.
\end{align*}
\end{example}

\begin{definition}[Coeficiente de correlação]
Sejam $X,Y$ v.a.'s em $(\Omega, \mathcal{A}, P)$ com variância finita e positiva. O \textbf{coeficiente de correlação} entre $X$ e $Y$ é definido por
\begin{align*}
    \rho(X,Y) = \frac{\Cov(X,Y)}{\sqrt{\Var(X)}\sqrt{\Var(Y)}} = \frac{\Cov(X,Y)}{\sigma_X\sigma_Y}.
\end{align*}
\end{definition}

\begin{remark}
Note que $X, Y$ são não-correlacionadas se, e só se, $\rho(X,Y) = 0$. Além disso, se $X,Y$ são independentes, então $\rho(X,Y) = 0$.
\end{remark}

\begin{theorem}[Desigualdade de Cauchy-Schwarz]
Sejam $X,Y$ v.a.'s com segundo momento finito. Então
\begin{align*}
    E[|XY|] \leq \sqrt{E[X^2]E[Y^2]},
\end{align*}
com igualdade se, e só se, $P(Y=aX) = 1,$ para algum $a\in\mathbb{R}$.
\end{theorem}

\begin{proof}
Se $E[X^2] = 0$ ou $E[Y^2] = 0$, então $P(X=0) = 1$ ou $P(Y=0) = 1$, de modo que $P(XY=0) = 1$ e $E[XY] = 0 = E[X^2]E[Y^2]$. Se $E[X^2], E[Y^2] > 0$, temos
\begin{align*}
    &E\left[ \left( \frac{|X|}{\sqrt{E[X^2]}} - \frac{|Y|}{\sqrt{E[Y^2]}} \right)^2 \right] \geq 0 \\
    &\iff 2 - 2\frac{E[|XY|]}{\sqrt{E[X^2]E[Y^2]}} \geq 0 \\
    &\iff E[|XY|] \leq \sqrt{E[X^2]E[Y^2]}.
\end{align*}
A igualdade ocorre se, e só se, 
\begin{align*}
    P\left( \frac{|X|}{\sqrt{E[X^2]}} - \frac{|Y|}{\sqrt{E[Y^2]}} = 0 \right) = 1 &\iff P\left( |Y| = \frac{ \sqrt{E[Y^2]} }{ \sqrt{E[X^2]}|X| } \right) = 1 \\
    &\iff P(Y = aX) = 1, a = \pm\sqrt{\frac{ E[Y^2] }{ E[X^2] }} \in\mathbb{R}.
\end{align*}
\end{proof}

\begin{proposition}
Sejam $X, Y$ v.a.'s com variância finita e positiva. Então
\begin{enumerate}[(i)]
    \item $|\rho(X,Y)| \leq 1$
    \item $|\rho(X,Y)| = 1 \iff P(Y = aX + b) = 1$, para algum par $a,b\in\mathbb{R}$. Nesse caso, $\rho(X,Y) = 1$ se $a>0$ e $\rho(X,Y) = -1$ se $a<0$.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[(i)]
    \item Temos 
    \begin{align*}
        |E[(X-\mu_X)(Y-\mu_Y)]| \leq E[|(X-\mu_X)(Y-\mu_Y)|] \leq \sqrt{ E[(X-\mu_X)^2]E[(Y-\mu_Y)^2] },
    \end{align*}
    ou seja,
    \begin{align*}
        |\Cov(X,Y)| \leq \sigma_X\sigma_Y \iff |\rho(X,Y)| \leq 1.
    \end{align*}
    \item Se $\rho(X,Y) = \pm 1$, então
    \begin{align*}
        &\Cov(X,Y) = \pm\sigma_X\sigma_Y \\
        &\implies E[(X-\mu_X)(Y-\mu_Y)] = \pm\sqrt{ E[(X-\mu_X)^2]E[(Y-\mu_Y)^2] } \\
        &\implies P(Y - \mu_Y = a(X-\mu_X)) = 1,
    \end{align*}
    ou seja, 
    \begin{align*}
        P(Y = aX+b) = 1, a = \pm\sqrt{ \frac{ E[(Y-\mu_Y)^2] }{ E[(X-\mu_X)^2] } }, b = \mu_Y - a\mu_X.
    \end{align*}
    Se $P(Y = aX+b) = 1$, então $\mu_Y = a\mu_X + b$ e $\sigma_Y^2 = a^2\sigma_X^2$. Logo,
    \begin{align*}
        \Cov(X,Y) = E[XY] - \mu_X\mu_Y = E[X(aX+b) - \mu_X(a\mu_X + b)] = a\sigma_X^2,
    \end{align*}
    de modo que
    \begin{align*}
        \rho(X,Y) = \frac{ a\sigma_X^2 }{ \sigma_X|a|\sigma_X } = \frac{a}{|a|} = \begin{cases}
        1, a>0 \\
        -1, a<0
        \end{cases}.
    \end{align*}
\end{enumerate}
\end{proof}

\begin{remark}
Como mencionamos, note que $\rho(X,Y)$ é uma medida do grau de dependência \textbf{linear} entre $X$ e $Y$: $\rho(X,Y)\approx\pm 1$ indica alta linearidade entre $X$ e $Y$, com $\rho(X,Y) > 0$ indicando que $Y$ cresce quando $X$ cresce e $\rho(X,Y) < 0$ indicando que $Y$ decresce quando $X$ cresce; $\rho(X,Y) \approx 0$ indica ausência/fraca de linearidade entre $X$ e $Y$. Se $X$ e $Y$ são independentes, então elas são não-correlacionadas pois não existe nenhuma relação (em particular, nenhuma relação linear) entre $X$ e $Y$. Entretanto, a recíproca é \textbf{falsa!} Por exemplo,
\begin{align*}
    P(Y=X^2) = 1 \implies \begin{cases}
    \rho(X,Y) = 0 \\
    X, Y \text{ não independentes}
    \end{cases}.
\end{align*}
Observe que, nesse caso, a dependência entre $X$ e $Y$ não é linear.
\end{remark}


\end{document}