\documentclass[../Notas.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

%\section{Tópico 4}

\section{Esperança de variáveis aleatórias discretas}
Começamos com uma motivação para depois introduzirmos a definição de esperança: dada uma amostra $\{ a_1, a_2, \dots, a_n \}\subset\mathbb{R}$, a média amostral é dada por $\displaystyle{ \frac{a_1 + a_2 + \cdots + a_n}{n} }$. Podemos representar esses dados numa tabela:
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        Valores distintos na amostra & Frequência absoluta \\
        \hline
        $x_1$ & $N_1$ \\
        $x_2$ & $N_2$ \\
        \vdots & \vdots \\
        $x_m$ & $N_m$
    \end{tabular}
%    \caption{Caption}
%    \label{tab:my_label}
\end{table}
Note que $N_i$ é o número de vezes que $x_i$ aparece na amostra, $i= 1,2,\dots, m$. Além disso, $N_1 + N_2 + \cdots + N_m = n$ e $x_1N_1 + x_2N_2 + \cdots + x_mN_m = a_1 + a_2 + \cdots + a_n$. Com isso, podemos escrever a média dos $a_i$ como $\displaystyle{ \frac{x_1N_1 + x_2N_2 + \cdots + x_mN_m}{N_1 + N_2 + \cdots + N_m} }$ ou, ainda, $\displaystyle{ x_1\frac{N_1}{n} + \cdots + x_m\frac{N_m}{n} }$, em que $N_i/n$ é a frequência relativa de $x_i$, $i=1,2,\dots,m$. Nesse caso, a f.p. $p:\mathbb{R}\to\mathbb{R}$ é dada por
\begin{align*}
    p(x) = \begin{cases}
    N_i/n, x=x_i \text{ para algum } i=1,2,\dots,m \\
    0, \text{ c.c.}
    \end{cases}.
\end{align*}
Agora, suponhamos que $x_1, x_2, \dots, x_m$ sejam valores possíveis de uma v.a. $X$ e que $a_1, a_2, \dots, a_n$ são valores (independentes) observados de $X$. Então, de acordo com a interpretação da probabilidade como frequência relativa (Lei Forte dos Grandes Números), temos que para $n$ grande a frequência relativa se aproxima da probabilidade real, i.e.,
\begin{align*}
    \lim_{n\to\infty}\frac{N_i}{n} = p(x_i) = P(X=x_i).
\end{align*}
Assim, o \textbf{valor esperado} de $X$, representado por $EX$ ou $E[X]$, é $\displaystyle{ EX = \sum_{i=1}^{m}x_i p(x_i) }$, sendo $p$ a f.p. de $X$.

\begin{definition}[Esperança]
Seja $X$ uma v.a. discreta em $(\Omega, \mathcal{A}, P)$ com valores possíveis $\{ x_1, x_2, \dots \}$. Se $\displaystyle{ \sum_{i} |x_i|p(x_i) < \infty }$, dizemos que $X$ tem \textbf{esperança finita} e definimos sua esperança ou \textbf{esperança matemática} ou \textbf{valor esperado} ou \textbf{média} como
\begin{align*}
    EX = \sum_{i=1}^{\infty}x_ip(x_i) = \sum_x xp(x).
\end{align*}
Note, em particular, que se o conjunto dos valores possíveis de $X$ é finito, então $X$ tem esperança finita.
\end{definition}

\begin{example}
Seja $X\sim B(n,p)$. Temos
\begin{align*}
    p_X(k) = \begin{cases}
    \binom{n}{k}p^k(1-p)^{n-k}, k = 0,1,\dots,n \\
    0, \text{ c.c.}
    \end{cases}.
\end{align*}
Como $X$ tem uma quantidade finita de valores possíveis, então $X$ tem esperança finita dada por
\begin{align*}
    EX = \sum_x xp(x) = \sum_{k=0}^{n} kP(X=k) = \sum_{k=1}^{n} k\binom{n}{k}p^k(1-p)^{n-k} &= \sum_{k=1}^{n} \frac{n!}{(k-1)!(n-k)!}p^k(1-p)^{n-k} \\
    &= np\sum_{k=1}^{n} \binom{n-1}{k-1}p^{k-1}(1-p)^{n-1-(k-1)} \\
    &= np.
\end{align*}
Em particular, se $n=1$ então $EX = p$.
\end{example}

\begin{example}
Seja $X\sim\text{Poisson}(\lambda)$. Temos
\begin{align*}
    p_X(k) = \begin{cases}
    e^{-\lambda}\frac{\lambda^k}{k!}, k = 0,1,\dots \\
    0, \text{ c.c.}
    \end{cases}.
\end{align*}
Note que como $X$ tem infinitos valores possíveis, não sabemos a priori se $EX < \infty$. Temos
\begin{align*}
    \sum_x |x|p_X(x) = \sum_{k=0}^{\infty} |k|e^{-\lambda}\frac{\lambda^k}{k!} = \lambda e^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!} = \lambda < \infty.
\end{align*}
Logo, $X$ tem esperança finita e, como seus valores possíveis são não negativos, segue que $EX=\lambda$.
\end{example}

\begin{example}
Seja $X\sim\text{Geom}(p)$. Temos
\begin{align*}
    p_X(k) = \begin{cases}
    p(1-p)^{k-1}, k = 1,2,\dots \\
    0, \text{ c.c.}
    \end{cases}, 0 < p < 1.
\end{align*}
Note que
\begin{align*}
    \sum_x |x|p_X(x) = \sum_{k=1}^{\infty} |k|p(1-p)^{k-1} = p\sum_{k=1}^{\infty} k(1-p)^{k-1} &= -p\sum_{k=1}^{\infty} \dfrac{d}{dp}[(1-p)^k] \\
    &= -p\dfrac{d}{dp}\left[ \sum_{k=1}^{\infty} (1-p)^k \right] \\
    &= -p(-1/p^2) \\
    &= 1/p,
\end{align*}
em que a série converge pelo teste da integral. Logo, $X$ tem esperança finita e, como seus valores possíveis são não negativos, temos $EX=1/p$.
\end{example}

\begin{example}
Considere $p:\mathbb{R}\to\mathbb{R}$ dada por
\begin{align*}
    p(x) = \begin{cases}
    \frac{1}{x(x+1)}, x = 1,2,\dots \\
    0, \text{ c.c.}
    \end{cases}
\end{align*}
Note que $p(x)\geq 0, \forall x\in\mathbb{R}$, $\{x\in\mathbb{R} : p(x) > 0\} = \mathbb{N}$ é enumerável e $\displaystyle{ \sum_x p(x) = \sum_{x=1}^{\infty}\frac{1}{x(x+1)} = \lim_{N\to\infty}\sum_{x=1}^N \frac{1}{x} - \frac{1}{x+1} = \lim_{N\to\infty} 1 - \frac{1}{N+1} = 1 }$. Logo, $p$ é uma f.p. e, se $X$ é v.a. com f.p. $p$, temos
\begin{align*}
    \sum_x |x|p(x) = \sum_{x=1}^{\infty}\frac{1}{x+1}
\end{align*}
que diverge pelo teste da integral. Logo, $X$ não tem esperança finita e escrevemos $EX = +\infty$.
\end{example}

\subsection{Esperança de função de variável aleatória}
Nosso interesse aqui é determinar a esperança de uma v.a. (discreta) que é função de uma ou mais v.a.'s: $Z = g(X)$, com $X = (X_1, \dots, X_n)$ e $g:\mathbb{R}^n\to\mathbb{R}$ função. Note que o conjunto de valores possíveis de $X$ é $\{x_1, x_2, \dots \}\subset\mathbb{R}^n$ (enumerável), ou seja, $P(X=x_j) > 0, j = 1,2,\dots$ e $P(X\in\{x_1, x_2, \dots\}) = 1$. Note que $x_j = (x_{j_1}, \dots, x_{j_n})\in\mathbb{R}^n, j=1,2,\dots$ e $\displaystyle{ \sum_x g(x)p_X(x) = \sum_j g(x_j)p_X(x_j) }$.

\begin{theorem}
Sejam $X = (X_1, \dots, X_n)$ um vetor aleatório discreto com f.p. $p_X$ e $g:\mathbb{R}^n\to\mathbb{R}$ função. A v.a. $Z = g(X)$ tem esperança finita se, e só se, $\displaystyle{ \sum_x |g(x)|p_X(x) < \infty }$. Nesse caso,
\begin{align*}
    EZ = \sum_x g(x)p_X(x) = \sum_{x_1}\cdots\sum_{x_n} g(x_1, \dots, x_n)P(X_1 = x_1, \dots, X_n=x_n).
\end{align*}
\end{theorem}

\begin{proof}
Vamos mostrar o caso $n=1$; o caso $n>1$ é análogo. Temos $g:\mathbb{R}\to\mathbb{R}$ função e, se $\{x_1, x_2, \dots\}$ é o conjunto de valores possíveis de $X$, então $\{ g(x_1), g(x_2), \dots, g(x_n) \}$ é o conjunto de valores possíveis de $Z = g(X)$. Note que pode haver repetição, i.e., podemos ter $g(x_i) = g(x_k)$ com $x_i\neq x_k$. Seja $A_j = \{ x : x=x_i \text{ e } g(x) = z_j \}, j = 1,2,\dots$, sendo $\{z_1, z_2, \dots\}$ o conjunto de valores possíveis de $Z$. Note que $\{X\in A_j\} = \{Z=z_j\}$ e, para cada $j=1,2,\dots$, $A_j\subset\{ x_1, x_2, \dots \}$ e os $A_j$'s são dois a dois disjuntos já que $g$ função. Além disso,
\begin{align*}
    \bigcup_{j=1}^{\infty}A_j = \{x_1, x_2, \dots\}.
\end{align*}
Logo, 
\begin{align*}
\sum_j |z_j|p_Z(z_j) = \sum_j |z_j|P(X\in A_j) &= \sum_j |z_j|\sum_{i : x_i\in A_j} P(X=x_i) \\
&= \sum_j\sum_{i:x_i\in A_j} |z_j|P(X=x_i) \\
&= \sum_j\sum_{i:x_i\in A_j}|g(x_i)|P(X=x_i) \\
&= \sum_k |g(x_k)|P(X=x_k).
\end{align*}
Logo, $EZ < \infty \iff \displaystyle{ \sum_x |g(x)|p_X(x) < \infty }$. Agora, se $EZ < \infty$, então analogamente ao acima, $EZ = \displaystyle{ \sum_x g(x)p_X(x) }$.
\end{proof}

\begin{remark}
Dada uma v.a. discreta $X$, vimos que (i) $EX < \infty \iff \displaystyle{ \sum_x |x|P(X=x) < \infty }$ (por definição) e (ii) $E|X|$, quando existe, é $\displaystyle{ \sum_x |x|P(X=x)}$ pelo teorema acima. Logo, $EX < \infty \iff E|X| < \infty$.
\end{remark}

\begin{example}
Sejam $X, Y$ v.a.'s independentes com $X\sim\text{Poisson}(\lambda_1)$ e $Y\sim\text{Poisson}(\lambda_2)$. Vamos calcular $E[X^2], E[XY]$ e $E[X+Y]$.
\begin{enumerate}[(i)]
    \item Pelo teorema,
    \begin{align*}
        \sum_x |g(x)|p_X(x) &= \sum_{k=0}^{\infty} k^2e^{-\lambda_1}\frac{\lambda_1^k}{k!} \\
        &= \sum_{k=1}^{\infty} ke^{-\lambda_1}\frac{\lambda_1^{k}}{(k-1)!} \\
        &= \lambda_1\left[ \sum_{k=1}^{\infty}(k-1)e^{-\lambda_1}\frac{\lambda_1^{k-1}}{(k-1)!} + \sum_{k=1}^{\infty}e^{-\lambda_1}\frac{\lambda_1^{k-1}}{(k-1)!} \right] \\
        &= \lambda_1[EX + e^{\lambda_1}e^{-\lambda_1}] \\
        &= \lambda_1(\lambda_1 + 1)\in\mathbb{R}.
    \end{align*}
    Logo, $X^2$ tem esperança finita e, como seus valores possíveis são não negativos, $E[X^2] = \lambda_1(\lambda_1 + 1)$.
    
    \item Pelo teorema,
    \begin{align*}
        \sum_x\sum_y |g(x,y)|p_{X,Y}(x,y) &= \sum_{k=0}^{\infty}\sum_{j=0}^{\infty}kjP(X=k, Y=j) \\
        &= \sum_{k=1}^{\infty}\sum_{j=1}^{\infty} kje^{-\lambda_1}\frac{\lambda_1^k}{k!}e^{-\lambda_2}\frac{\lambda_2^j}{j!} \\
        &= \sum_{k=1}^{\infty}\left[ ke^{-\lambda_1}\frac{\lambda_1^k}{k!}\sum_{j=1}^{\infty}je^{-\lambda_2}\frac{\lambda_2^j}{j!} \right] \\
        &= \lambda_2\sum_{k=1}^{\infty} ke^{-\lambda_1}\frac{\lambda_1^k}{k!} \\
        &= \lambda_1\lambda_2\in\mathbb{R},
    \end{align*}
    logo $XY$ tem esperança finita e $E[XY] = \lambda_1\lambda_2$ pois os valores possíveis de $XY$ são não negativos.
    
    \item Temos, pelo teorema, que
    \begin{align*}
        \sum_x\sum_y |g(x,y)|p_{X,Y}(x,y) &= \sum_{k=0}^{\infty}\sum_{j=0}^{\infty} (k+j)P(X=k, Y=j) \\
        &= \sum_{k=0}^{\infty}k\sum_{j=0}^{\infty} P(X=k, Y=j) + \sum_{j=0}^{\infty}j\sum_{k=0}^{\infty} jP(X=k, Y=j) \\
        &= \sum_{k=0}^{\infty}kp_X(k) + \sum_{j=0}^{\infty} jp_Y(j) \\
        &= EX + EY \\
        &= \lambda_1 + \lambda_2\in\mathbb{R},
    \end{align*}
    logo $X+Y$ tem esperança finita e $E[X+Y] = EX + EY = \lambda_1 + \lambda_2$ pois os valores possíveis de $X+Y$ são não negativos.
\end{enumerate}
\end{example}

\begin{theorem}[Propriedades da esperança]
Sejam $X$ e $Y$ v.a.'s definidas em $(\Omega, \mathcal{A}, P)$ com $EX, EY < \infty$. Temos
\begin{enumerate}[(a)]
    \item $P(X=c) = 1\implies EX = c, \forall c\in\mathbb{R}$
    \item $E[cX] < \infty$ e $E[cX] = cEX, \forall c\in\mathbb{R}$
    \item $E[X+Y] < \infty$ e $E[X+Y] = EX+EY$. De maneira geral, dados $c_1, \dots, c_n\in\mathbb{R}$ e $EX_1, \dots, EX_n < \infty$, então
    \begin{align*}
        E[c_1X_1 + \cdots + c_nX_n] = \sum_i c_iEX_i < \infty
    \end{align*}
    \item Se $P(X\geq Y) = 1$, então $EX\geq EY$ e $EX = EY\iff P(X=Y) = 1$. Em particular, $P(X\geq 0) = 1 \implies EX \geq 0$
    \item $|EX|\leq E|X|$.
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}[(a)]
    \item Temos $\displaystyle{ \sum_x |x|p(x) = |c| < \infty } \therefore EX = c < \infty$.
    \item Temos $\displaystyle{ \sum_x |cx|p(x) = |c|\sum_x |x|p(x) < \infty \therefore E[cX] = cEX < \infty }$.
    \item Temos
    \begin{align*}
        &\sum_{x_1}\cdots\sum_{x_n}|c_1x_1 + \cdots + c_nx_n|P(X_1=x_1, \dots, X_n=x_n) \\
        &\leq \sum_{x_1}|c_1x_1|\sum_{x_2}\cdots\sum_{x_n}P(X_1=x_1, \dots, X_n=x_n) + \cdots + \sum_{x_n}|c_nx_n|\sum_{x_1}\cdots\sum_{x_{n-1}}P(X_1=x_1, \dots, X_n=x_n) \\
        &= |c_1|\sum_{x_1}|x_1|P(X_1=x_1) + \cdots + |c_n|\sum_{x_n}|x_n|P(X_n=x_n) < \infty.
    \end{align*}
    Logo, $\displaystyle{ E\left[ \sum_{i}c_iX_i \right] = \sum_i c_iEX_i < \infty .}$
    
    \item Se $P(X\geq Y) = 1$, então $x_i\geq y_j$ para todo par de valores possíveis $(x_i, y_j)$. Daí,
    \begin{align*}
        EX = \sum_x xP(X=x) \geq \sum_y yP(X=x) \geq \sum_y yP(Y=y) = EY.
    \end{align*}
    Se $P(X=Y) = 1$, então $X$ e $Y$ têm o mesmo conjunto de valores possíveis e $P(X=x) = P(Y=y)$. Logo, $EX = \sum_x xP(X=x) = \sum_y yP(Y=y) = EY$. Se $EX=EY$, devemos ter $X$ e $Y$ com o mesmo conjunto de valores possíveis e $P(X=x) = P(Y=y)$.
    
    \item $\displaystyle{|EX| = \left| \sum_x xp(x) \right| \leq \sum_x |x|p(x) = E|X|}$.
\end{enumerate}
\end{proof}

\begin{theorem}
Seja $X$ v.a. definida em $(\Omega, \mathcal{A}, P)$ tal que $P(|X|\leq M) = 1, M\in\mathbb{R}$. Então $EX < \infty$ e $|EX|\leq M$.
\end{theorem}

\begin{proof}
Seja $x$ valor possível de $X$. Se $|x|>M$, então $P(|X|>M)\geq P(X=x)>0$. Mas $P(|X|>M) = 1 - P(|X|\leq M) = 0$. Logo, $|x|\leq M$ e, assim,
\begin{align*}
    \sum_x |x|p_X(x) \leq M\sum_x p_X(x) = M < \infty,
\end{align*}
ou seja, $EX < \infty$. Por fim,
\begin{align*}
    |EX| \leq E|X| \leq EM = M.
\end{align*}
\end{proof}

\begin{example}
Se $X\sim B(n,p)$, então $EX = np$, como vimos. Outra maneira de calcular $EX$ seria tomar $X_i\sim B(1,p), i=1,\dots, n$ e $S_n = X_1 + \cdots + X_n\sim B(n,p)$. Então $ES_n = E[X_1 + \cdots + X_n] = \sum_i EX_i = np$.
\end{example}

\begin{example}
Seja $X\sim\text{Hgeo}(N,m,n)$. Temos
\begin{align*}
    p_{S_n}(k) = \begin{cases}
    \frac{ \binom{m}{k}\binom{N-m}{n-k} }{ \binom{N}{n} }, k = 0,1,\dots,n \\
    0, \text{ c.c.}
    \end{cases}.
\end{align*}
Poderíamos calcular $ES_n$ pela definição, mas isso seria trabalhoso e complicado; vamos proceder de outro modo. Sejam $X_i$ v.a.'s indicadoras da presença do objeto de tipo $i$. Temos, então,
\begin{align*}
    EX_i = \frac{ \binom{1}{1}\binom{N-1}{n-1} }{\binom{N}{n}} = \frac{n}{N}, i = 1, 2, \dots, n.
\end{align*}
Logo, $ES_n = \sum_i EX_i = mn/N$.
\end{example}

\begin{remark}
Vimos que $EX, EY < \infty$ implica $E[X+Y] = EX+EY$. Contudo, o mesmo não vale para $E[XY]$. De fato, sejam $X,Y$ v.a.'s com $p_X(-1) = 1/2 = p_X(1)$ e $Y=X$. A f.p. conjunta de $X$ e $Y$ pode ser representada como
\begin{table}[H]
    \centering
    \begin{tabular}{c|c|c|c}
        $X\setminus Y$ & -1 & 1 & $p_X(x)$ \\
        \hline
        -1 & 1/2 & 0 & 1/2 \\
        1 & 0 & 1/2 & 1/2 \\
        \hline
        $p_Y(y)$ & 1/2 & 1/2 & 1
    \end{tabular}
%    \caption{Caption}
%    \label{tab:my_label}
\end{table}
Temos $EX = -1/2 + 1/2 = 0 = EY$ mas $E[XY] = 1/2 + 1/2 = 1 \neq EXEY$. É importante notar que $X$ e $Y$ \textbf{não são independentes!}
\end{remark}

\begin{theorem}
Sejam $X$ e $Y$ v.a.'s definidas em $(\Omega, \mathcal{A}, P)$ com $EX, EY <\infty$. Se $X$ e $Y$ são independentes, então $E[XY] < \infty$ e $E[XY] = EXEY$.
\end{theorem}

\begin{proof}
Temos
\begin{align*}
    \sum_x\sum_y |xy|p_{X,Y}(x,y) = \sum_x |x|p_X(x)\sum_y |y|p_Y(y) < \infty
\end{align*}
pois $EX, EY < \infty$. Daí, segue que
\begin{align*}
    E[XY] = \left( \sum_x xp_X(x) \right)\left( \sum_y yp_Y(y) \right) = EXEY.
\end{align*}
\end{proof}

\begin{remark}
Note que a recíproca deste teorema \textbf{é falsa!} O exemplo anterior é um contraexemplo disso.
\end{remark}

\begin{theorem}
Seja $X$ v.a. inteira não negativa, i.e., com conjunto de valores possíveis $\{0, 1, 2, \dots\}$. Então $EX < \infty \iff \displaystyle{ \sum_{x=1}^{\infty} P(X\geq x) < \infty }$ e, nesse caso, $EX = \displaystyle{ \sum_{x=1}^{\infty} P(X\geq x) = \sum_{x=0}^{\infty} 1 - F_X(x) }$. 
\end{theorem}

\begin{proof}
Temos que $EX < \infty$ se, e só se,
\begin{align*}
    \sum_x |x|p_X(x) = \sum_{x=1}^{\infty} xp_X(x) = \sum_{x=1}^{\infty}p_X(x) + \sum_{x=2}^{\infty}p_X(x) + \cdots = \sum_{k=1}^{\infty}\sum_{x=k}^{\infty}p_X(x) = \sum_{k=1}^{\infty}P(X\geq k) < \infty,
\end{align*}
em que a segunda igualdade vale pois a convergência é absoluta. Daí, quando existe, $EX$ é dada por
\begin{align*}
    \sum_{k=1}^{\infty} P(X\geq k) = \sum_{k=0}^{\infty} 1 - F_X(k).
\end{align*}
\end{proof}

\begin{example}
$X\sim\text{Geo}(p)$ é inteira não negativa, logo
\begin{align*}
    EX = \sum_{k=1}^{\infty} P(X\geq k) = \sum_{k=0}^{\infty} 1 - F_X(k) = \sum_{k=0}^{\infty} (1-p)^k = 1/p.
\end{align*}
\end{example}

\begin{example}
Sejam $X, Y\sim\text{Geom}(p)$ e $W = \min\{X,Y\}$. Temos $W$ inteira não negativa e
\begin{align*}
    F_W(w) = \begin{cases}
    0, w < 1 \\
    1 - (1-p)^{2[w]}, w\geq 1
    \end{cases}.
\end{align*}
Logo, 
\begin{align*}
    EW = \sum_{k=0}^{\infty} 1 - F_W(k) = \sum_{k=0}^{\infty} (1-p)^{2k} = \frac{1}{1-(1-p)^2} = \frac{1}{2p-p^2} = \frac{1}{p(2-p)}.
\end{align*}
\end{example}

\subsection{Momentos}
\begin{definition}[Momentos e variância]
Sejam $X$ v.a. e $k\in\mathbb{N}$. Se $E[X^k]<\infty$, dizemos que $X$ tem \textbf{momento de ordem} $k$ e chamamos $E[X^k]$ de \textbf{momento de ordem} $k$ ou $k$-ésimo momento de $X$. Quando $X$ tem momento de ordem $k$, chamamos $E[(X-b)^k]$ de \textbf{momento de ordem} $k$ em torno de $b, \forall b\in\mathbb{R}$. Em particular, se $k=2$ e $b=EX$, temos
\begin{align*}
    E[(X-EX)^2] := \Var(X) = \sigma_X^2 \ \text{ e } \ \sqrt{\Var(X)} := \sigma_X
\end{align*}
a \textbf{variância} e o \textbf{desvio-padrão} de $X$, respectivamente. É comum denotar também $EX = \mu$.
\end{definition}

\begin{proposition}[Propriedades dos momentos]
Os momentos de uma v.a. satisfazem as seguintes propriedades.
\begin{itemize}
    \item[(P1)] Se $X$ é v.a. com momento de ordem $r$, então $X$ tem momento de ordem $k$ para todo $k\leq r$.
    \item[(P2)] Se $X$ e $Y$ são v.a.'s com momento de ordem $r$, então $X+Y$ também tem momento de ordem $r$ e, de maneira geral, se $X_1, \dots, X_n$ são v.a.'s com momento de ordem $r$, então $X_1+\cdots+X_n$ também tem momento de ordem $r$ e, se $E[X^2] < \infty$, então $\Var(X) < \infty$.
    \item[(P3)] Seja $X$ v.a. com segundo momento finito. Então
    \begin{enumerate}[(i)]
        \item $\Var(X) \geq 0$
        \item $\Var(X) = E[X^2] - EX^2$
        \item $P(X=c) = 1 \iff \Var(X) = 0$
        \item $\Var(X+c) = \Var(X), \forall c\in\mathbb{R}$
        \item $\Var(cX) = c^2\Var(X), \forall c\in\mathbb{R}$
        \item Se $Y$ é v.a. com segundo momento finito, então $\Var(X+Y) = \Var(X) + \Var(Y) + 2E[(X-EX)(Y-EY)] < \infty$.
    \end{enumerate}
\end{itemize}
\end{proposition}

\begin{proof}
\begin{itemize}
    \item[(P1)] Da definição de esperança, temos $E[X^r] = \displaystyle{ \sum_x x^rp_X(x) < \infty }$ e essa série converge absolutamente. Logo, podemos derivá-la, obtendo os demais momentos.
    
    \item[(P2)] Note que $E[Y^r] < \infty \implies E[(-Y)^r] = (-1)^rE[Y^r] <\infty$, logo $-Y$ tem momento de ordem $k$. Note que
    \begin{align*}
        E[X^2] < \infty \implies EX < \infty \implies E[(EX)^2] = EX^2,
    \end{align*}
    logo $EX$ tem segundo momento finito; como $X$ e $EX$ têm segundo momento finito, então $X-EX$ também tem e $\Var(X) < \infty$.
    
    \item[(P3)]\begin{enumerate}[(i)]
        \item Temos
        \begin{align*}
            \Var(X) = E[(X-EX)^2] \geq 0
        \end{align*}
        pois $(X-EX)^2$ é não negativa.
        
        \item Temos
        \begin{align*}
            \Var(X) = E[(X-EX)^2] &= E[X^2 - 2XEX + EX^2] \\
            &= E[X^2] - 2EXEX + EX^2 \\
            &= E[X^2] - EX^2.
        \end{align*}
        
        \item Note que
        \begin{align*}
            P(X=c) = 1 \implies P(X^2=c^2) = 1 \implies E[X^2] = c^2 \therefore \Var(X) = c^2-c^2 = 0.
        \end{align*}
        Reciprocamente,
        \begin{align*}
            \Var(X) = 0 \implies E[(X-EX)^2] = 0 &\implies \sum_i \underbrace{(x_i - EX)^2}_{\geq 0} \underbrace{P(X=x_i)}_{> 0} = 0 \\
            &\implies x_i = EX, i=1,2,\dots \\
            &\implies EX = c \text{ é o único valor possível de } X \\
            &\therefore P(X=c) = 1.
        \end{align*}
        
        \item Temos
        \begin{align*}
            \Var(X+c) = E[(X+c - E[X+c])^2] &= E[(X + c - EX - c)^2] \\
            &= E[(X-EX)^2] \\
            &= \Var(X), \forall c\in\mathbb{R}.
        \end{align*}
        
        \item Temos
        \begin{align*}
            \Var(cX) = E[(cX - E[cX])^2] = E[c^2(X-EX)^2] = c^2\Var(X), \forall c\in\mathbb{R}.
        \end{align*}
        
        \item Temos
        \begin{align*}
            \Var(X+Y) &= E[(X+Y - E[X+Y])^2] \\
            &= E[(X+Y-EX-EY)^2] \\
            &= E[(X-EX)^2 + (Y-EY)^2 + 2(X-EX)(Y-EY)] \\
            &= \Var(X) + \Var(Y) +2E[(X-EX)(Y-EY)].
        \end{align*}
    \end{enumerate}
\end{itemize}
\end{proof}



\end{document}