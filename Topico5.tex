\documentclass[../Notas.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}

%\section{Tópico 5}

\section{Variáveis aleatórias contínuas}
Vimos situações em que as v.a.'s representavam o número de ``objetos'' ou ``coisas''. Entretanto, há muitas situações (tanto teóricas quanto práticas) em que a v.a. natural a se considerar é ``contínua'' num certo sentido, e.g. o tempo que decorre até a recuperação completa de um paciente com determinada doença.

\begin{definition}[V.a. contínua]
Uma v.a. $X$ em $(\Omega, \mathcal{A}, P)$ é dita \textbf{contínua} se $P(X=x) = 0, \forall x\in\mathbb{R}$.
\end{definition}

Recordando das propriedades da f.d. de uma v.a., temos o seguinte fato.

\begin{proposition}
$X$ é v.a. contínua se, e só se, $F_X$ é contínua.
\end{proposition}

\begin{proof}
Dado $x\in\mathbb{R}$,
\begin{align*}
    0 = P(X=x) = F_X(x) - F_X(x^-) \iff F_X(x^+) = F_X(x) = F_X(x^-).
\end{align*}
\end{proof}

No caso de v.a.'s contínuas, podemos trocar os sinais $<$ e $\leq$ à vontade nos cálculos de probabilidades.

\begin{example}
Considere o experimento de escolher um ponto ao acaso no círculo de centro na origem e raio $R>0$ (jogar um dardo). Vimos que, nesse caso, um espaço de probabilidade adequado é $(\Omega, \mathcal{A}, P)$ com
\begin{align*}
    \Omega = \{ (x,y)\in\mathbb{R}^2 : x^2+y^2\leq R^2 \}, \mathcal{A} = \mathcal{B}^2(\Omega)
\end{align*}
e $P:\mathcal{A}\to\mathbb{R}$ tal que
\begin{align*}
    P(A) = \iint_A \frac{1}{\pi R^2}dxdy, \forall A\in\mathcal{A}.
\end{align*}
Podemos definir a v.a. $X:\Omega\to\mathbb{R}$ por $X(\omega) = X((x,y)) = \sqrt{x^2+y^2}$. Note que, dado $z\in\mathbb{R}$, temos $\{X = z\} = \emptyset$ se $z < 0$ ou $z>R$ e $\{ X=z \} = \{ (x,y)\in\Omega : x^2+y^2 = z^2 \}$ se $0\leq z \leq R$. Logo, $P(X=z) = 0, \forall z\in\mathbb{R}$, ou seja, $X$ é v.a. contínua. Por outro lado, $\{X\leq z\} = \emptyset$ se $z<0$, $\{X\leq z\} = \{ (x,y)\in\Omega : x^2+y^2 \leq z^2 \}$ se $0\leq z < R$ e $\{X\leq z\} = \Omega$ se $z\geq R$. Logo,
\begin{align*}
    F_X(z) = \begin{cases}
    0, z < 0 \\
    z^2/R^2, 0\leq z < R \\
    1, z\geq R
    \end{cases}.
\end{align*}
Em particular, se $0\leq a < b\leq R$, então
\begin{align*}
    P(a < X < b) = F_X(b) - F_X(a) = \frac{b^2 - a^2}{R^2} > 0.
\end{align*}
\begin{center}
    \textbf{GRÁFICO P.104}
\end{center}
\end{example}

\begin{definition}[Função de densidade]
Uma função $f:\mathbb{R}\to\mathbb{R}$ tal que
\begin{enumerate}[(i)]
    \item $f(x)\geq 0, \forall x\in\mathbb{R}$ 
    \item $\displaystyle{ \int_{\mathbb{R}} f(x)dx  = 1}$
\end{enumerate}
é dita \textbf{função de densidade de probabilidade} ou apenas \textbf{densidade}.
\end{definition}

\begin{definition}[Função de distribuição]
Uma função $F:\mathbb{R}\to\mathbb{R}$ tal que
\begin{align*}
    F(x) = \int_{-\infty}^x f(t) dt, \forall x\in\mathbb{R}
\end{align*}
para alguma densidade $f$ é dita \textbf{função de distribuição (absolutamente) contínua}. Dizemos ainda que $f$ é a densidade de $F$.
\end{definition}

\begin{remark}
É possível, mas complicado, construir exemplos de funções $F$ que sejam contínuas mas não tenham densidade. As que têm densidade são chamadas de \textbf{absolutamente} contínuas. Aqui não faremos distinção, pois os casos não absolutamente contínuos são raros; sempre que nos referirmos a uma f.d., estará implícito que ela é absolutamente contínua.

Outro ponto importante é que dada $F$, a densidade $f$ não é única, já que $F$ pode não ser derivável. Contudo, os pontos onde $F$ não é derivável (ou onde $f$ não é contínua) formam um conjunto enumerável, de maneira que a integral não se altera. Em geral é comum tomar $f$ como
\begin{align*}
    f(x) = \begin{cases}
    F'(x), \forall x\in\mathbb{R} : \exists F'(x) \\
    0, \text{ c.c.}
    \end{cases}
\end{align*}
\end{remark}

\begin{definition}
Uma v.a. $X$ definida em $(\Omega, \mathcal{A}, P)$ é \textbf{(absolutamente) contínua} se sua f.d. $F_X$ é \textbf{(absolutamente) contínua}, i.e., se $F_X(x) = \displaystyle{ \int_{-\infty}^x f(t)dt, \forall x\in\mathbb{R} }$ para alguma função de densidade $f:=f_X$, chamada \textbf{densidade} de $X$.
\end{definition}

\begin{remark}
Como no caso discreto, podemos nos referir tanto a $F_X$ quanto a $f_X$ quando dizemos ``distribuição'', devido à relação biunívoca entre ambas as funções. Além disso, se $X$ é v.a. contínua com densidade $f_X$, então dados $a,b\in\mathbb{R}$ quaisquer com $a\leq b$, temos
\begin{align*}
    P(a < X < b) = F_X(b) - F_X(a) = \int_a^b f_X(x) dx,
\end{align*}
ou seja, $P(a < X < b)$ é a área da região $A$.
\begin{center}
    \textbf{GRÁFICO P.105}
\end{center}
\end{remark}

\begin{example}
No exemplo do dardo acima, vimos que
\begin{align*}
    F_X(x) = \begin{cases}
    0, x < 0 \\
    x^2/R^2, 0\leq x < R \\
    1, x\geq R
    \end{cases}.
\end{align*}
Logo, a densidade $f$ de $X$ é
\begin{align*}
    f_X(x) = \begin{cases}
    0, x < 0 \\
    2x/R^2, 0\leq x < R \\
    0, x\geq R
    \end{cases}.
\end{align*}
Note que não existe $F'_X(R)$, pois $F_{X_-}'(R) = 2/R \neq 0 = F'_{X_+}(R)$, ou seja, $f$ não é contínua em $R$. Assim, sem perda de generalidade, tomamos $f(R) = 0$, de modo que
\begin{align*}
    f_X(x) = \begin{cases}
    2x/R^2, 0\leq x < R \\
    0, \text{ c.c.}
    \end{cases}
\end{align*}
e ainda vale $\displaystyle{ F_X(x) = \int_{-\infty}^x f_X(t) dt, \forall x\in\mathbb{R}. }$
\end{example}

\begin{remark}
É importante notar que há v.a.'s que não são nem contínuas nem discretas, chamadas \textbf{mistas}. Por exemplo, a v.a. $X$ com f.d. $F_X$ dada pelo gráfico abaixo não é contínua, pois $F_X$ é descontínua em $a$; entretanto, $X$ tampouco é discreta, pois $F_X$ não é do tipo escada.
\begin{center}
    \textbf{GRÁFICO P.106}
\end{center}
\end{remark}

\begin{definition}
Seja $X$ uma v.a.; dizemos que
\begin{enumerate}[(i)]
    \item $X$ é simétrica em torno de 0 se $P(X\geq x) = P(X\leq -x), \forall x\in\mathbb{R}$, i.e., $X$ e $-X$ têm a mesma distribuição;
    \item $X$ é simétrica em torno de $\mu$ se existe $\mu\in\mathbb{R}$ tal que $P(X\geq \mu + x) = P(X\leq \mu - x), \forall x\in\mathbb{R}$.
\end{enumerate}
\end{definition}

\begin{theorem}
Seja $X$ v.a. contínua com densidade $f$. Então $X$ é simétrica em torno de 0 se, e só se, $f$ é par; nesse caso, $f$ é \textbf{densidade simétrica}. De modo geral, $X$ é simétrica em torno de $\mu\in\mathbb{R}$ se, e só se, $f(\mu + x) = f(\mu - x)$. Nesse caso, $f$ é \textbf{densidade simétrica em torno de} $\mu$.
\end{theorem}

\begin{proof}
Provamos em torno de 0. Se $f$ é par, então
\begin{align*}
    P(X\geq x) = \int_x^{\infty} f(t)dt = \int_{-\infty}^{-x} f(-y)dy = \int_{-\infty}^{-x} f(y)dy = P(X\leq -x),
\end{align*}
logo $X$ é simétrica. Reciprocamente, se $P(X\geq x) = P(X\geq -x)$, defina
\begin{align*}
    g(x) = \frac{f(x) + f(-x)}{2}.
\end{align*}
Note que $g$ é simétrica, logo
\begin{align*}
    \int_{-\infty}^x g(y) dy &= \frac{1}{2}\int_{-\infty}^x f(y) dy + \frac{1}{2}\int_{-\infty}^x f(-y) dy \\
    &= \frac{1}{2}\int_{-\infty}^x f(y) dy + \frac{1}{2}\int_{-x}^{\infty} f(y) dy \\
    &= \frac{1}{2}P(X\leq x) + \frac{1}{2}P(-X\geq -x) \\
    &= P(X\leq x),
\end{align*}
ou seja, $X$ tem densidade $g$ simétrica. A demonstração do caso geral é análoga, substituindo $x$ por $x+\mu$.
\end{proof}

\begin{center}
    \textbf{GRÁFICO P.106}
\end{center}

\begin{remark}
É possível mostrar que o resultado acima vale também para v.a.'s discretas; além disso, note que se $X$ é simétrica em torno da origem então $F(0) = 1/2$ e, se $X$ é simétrica em torno de $\mu$, então $F(\mu) = 1/2$. De maneira geral, se $X$ é simétrica em torno da origem então
\begin{align*}
    F(-x) = \int_{-\infty}^{-x}f(y) dy = \int_x^{\infty} f(-y)dy = \int_x^{\infty} f(y) dy = \int_{-\infty}^{\infty} f(y) dy - \int_{-\infty}^x f(y) dy,
\end{align*}
ou seja, $F(-x) = 1 - F_X(x), \forall x\in\mathbb{R}$.
\end{remark}

\subsection{Exemplos clássicos de distribuições contínuas}
\begin{example}[Uniforme contínua, $X\sim U(a,b)$]
Dizemos que $X$ é uma v.a. contínua com distribuição uniforme no intervalo $(a,b)$ se tem densidade dada por
\begin{align*}
    f(x) = \begin{cases}
    \frac{1}{b-a}, a < x < b \\
    0, \text{ c.c.}
    \end{cases}.
\end{align*}
Note que $f(x)\geq 0, \forall x\in\mathbb{R}$ e $\displaystyle{ \int_{\mathbb{R}} f(x) dx = 1 }$. A f.d. $F$ associada a $f$ é calculada como segue:
\begin{align*}
    F(x) &= \int_{-\infty}^x f(t) dt = 0, x < a \\
    F(x) &= \int_{-\infty}^x f(t) dt = \int_a^x \frac{1}{b-a} dt = \frac{x-a}{b-a}, a\leq x < b \\
    F(x) &= \int_{-\infty}^x f(t) dt = \int_a^b \frac{1}{b-a} dt = 1, x \geq b, 
\end{align*}
ou seja,
\begin{align*}
    F(x) = \begin{cases}
    0, x<a \\
    \frac{x-a}{b-a}, a\leq x < b \\
    1, x\geq b
    \end{cases}.
\end{align*}
Note também que $f(x) = F'(x), \forall x\in\mathbb{R}\setminus\{a,b\}$.
\end{example}

\begin{example}[Exponencial, $X\sim\text{Exp}(\lambda)$]
Dizemos que $X$ é uma v.a. contínua com distribuição exponencial de parâmetro $\lambda > 0$ se tem densidade dada por
\begin{align*}
    f(x) = \begin{cases}
    \lambda e^{-\lambda x}, x > 0 \\
    0, x \leq 0
    \end{cases}
\end{align*}
ou, equivalentemente, f.d. dada por
\begin{align*}
    F(x) = \begin{cases}
    1 - e^{-\lambda x}, x > 0 \\
    0, x\leq 0
    \end{cases}.
\end{align*}
Note que, de fato, $f(x) \geq 0, \forall x\in\mathbb{R}$ e que
\begin{align*}
    \int_{\mathbb{R}} f(t) dt = \int_0^{+\infty} \lambda e^{-\lambda t} dt = 1.
\end{align*}
Ademais, $f(x) = F'(x), \forall x\in\mathbb{R}\setminus\{0\}$.
\end{example}

\begin{remark}
A distribuição exponencial é utilizada muitas vezes quando a v.a. em questão é um tempo de espera, e.g. o tempo até que um componente eletrônico apresente falhas. Além disso, se $X\sim\text{Exp}(\lambda)$, então $X$ tem perda de memória, i.e., $P(X > a+b) = P(X>a)P(X>b), a,b\geq 0$ ou, equivalentemente, $P(X>a+b| X>a) = P(X>b), a,b\geq 0$.
\begin{proof}
De fato, se $X\sim\text{Exp}(\lambda)$, então
\begin{align*}
    P(X>a+b|X>a) &= \frac{P(X>a+b, X>a)}{P(X>a)} \\
    &= \frac{P(X>a+b)}{P(X>a)} \\
    &= \frac{ e^{-\lambda (a+b)} }{ e^{-\lambda a} } \\
    &= e^{-\lambda b} \\
    &= P(X>b), \forall a,b\geq 0.
\end{align*}
\end{proof}
Na verdade, a v.a. exponencial é a única v.a. contínua não negativa com perda de memória.
\end{remark}

\begin{theorem}
$X$ é v.a. contínua com perda de memória se, e só se, $X\sim\text{Exp}(\lambda)$ para algum $\lambda > 0$ ou $P(X>0) = 0$.
\end{theorem}

\begin{proof}
($\Leftarrow$) Já vimos o caso que $X$ tem distribuição exponencial; se $P(X>0) = 0$, então $P(X > a+b) = 0 = P(X>a)P(X>b), \forall a,b\geq 0$.

($\Rightarrow$) Se $X$ tem perda de memória e $P(X > 0) \neq 0$, então tomando $a = 0 = b$ segue que
\begin{align*}
    P(X>0) = [P(X>0)]^2 \implies P(X > 0) = 1, 
\end{align*}
ou seja, $X$ é v.a. positiva. Seja $F$ a f.d. de $X$ e defina $G(x) = 1 - F(x)$. Temos $G$ não crescente, contínua à direita, $G(0) = 1, G(+\infty) = 0$ e $G(a+b) = G(a)G(b), \forall a,b>0$. Daí, se $c\in\mathbb{R}^*_+$ e $m,n\in\mathbb{R}$, temos
\begin{align*}
    G(c) = G(c - c/m)G(c/m) = G(c - 2c/m)[G(c/m)]^2 = \cdots = G(0)[G(c/m)]^m = [G(c/m)]^m,
\end{align*}
donde segue que
\begin{align*}
    G(nc) = [G(c)]^n.
\end{align*}
Além disso, temos $0 < G(1) < 1$. De fato, se $G(1) = 1$ então teríamos
\begin{align*}
    G(n) = [G(1)]^n \implies G(+\infty) = 1,
\end{align*}
absurdo. Se $G(1) = 0$, então teríamos
\begin{align*}
    G(1/m) = 0 \implies G(0) = G(0^+) = \lim_{m\to +\infty} G(1/m) = \lim_{m\to +\infty} [G(1)]^{1/m} = 0,
\end{align*}
absurdo. Logo, como $0 < G(1) < 1$, podemos tomar $G(1) = e^{-\lambda}$, para algum $\lambda > 0$. Tomando $c=1$, segue que $G(1/m) = e^{-\lambda /m}, m\in\mathbb{N}$ e, fazendo $c = 1/m$, temos $G(n/m) = [G(1/m)]^n = e^{-\lambda n/m}, \forall n,m\in\mathbb{N}$. Logo, $G(y) = e^{-\lambda y}, \forall y\in\mathbb{Q}$. Da continuidade à direita, temos
\begin{align*}
    G(x) = \lim_{y\to x^+, y\in\mathbb{Q}} G(y) = e^{-\lambda x}, \forall x\in\mathbb{R}^*_+,
\end{align*}
pois $\mathbb{Q}$ é denso em $\mathbb{R}$. Daí, segue que $F(x) = 1 - e^{-\lambda x}, \forall x>0$, ou seja, $X\sim\text{Exp}(\lambda)$.
\end{proof}

\begin{remark}
Além de tempo de falha, variáveis exponenciais são úteis para estudar o tempo de decaimento de uma partícula radioativa e, também, no estudo de processos de Poisson e cadeias de Markov. Ademais, pensando na variável exponencial como o tempo de falha, a propriedade de perda de memória diz que dado que não houve falha até o tempo $a$, a probabilidade de que não haja falha nas próximas $b$ unidades de tempo é igual à probabilidade incondicional de que não haja falha nas primeiras $b$ unidades de tempo. Isso implica que o desgaste de uma peça de equipamento não aumenta nem diminui a probabilidade de falha em um dado intervalo de tempo.
\end{remark}

\begin{example}[Normal/gaussiana]
Dizemos que $X$ é v.a. com distribuição normal \textbf{padrão}, $X\sim N(0,1)$, se tem densidade dada por
\begin{align*}
    f(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}, x\in\mathbb{R}.
\end{align*}
\begin{proof}
Vamos mostrar que $f$ de fato é densidade. Primeiro, $f(x) > 0, \forall x\in\mathbb{R}$. Ademais, vamos verificar que $\displaystyle{ \int_{\mathbb{R}} f(x) dx = 1 }$. Seja $g(x) = e^{-x^2/2}, x\in\mathbb{R}$. Note que $g$ é par, contínua e não negativa. Além disso, se $x\geq 1$ então $0 < g(x) < e^{-x/2}$ e, então,
\begin{align*}
    \int_{1}^{\infty} e^{-x^2/2} dx \leq \int_{1}^{\infty} e^{-x/2} dx = \lim_{a\to +\infty} \int_{1}^{a} e^{-x/2} dx = \lim_{a\to +\infty} -2e^{-x/2}\Big|_{1}^a = 2e^{-1/2}\in\mathbb{R},
\end{align*}
logo $\displaystyle{ \int_{1}^{\infty} e^{-x^2/2} dx \in\mathbb{R}}$. Como $g$ é par, temos $\displaystyle{ \int_{1}^{\infty} e^{-x^2/2} dx = \int_{-\infty}^{-1} e^{-x^2/2} dx \in\mathbb{R}}$ e, como $g$ é contínua em $\mathbb{R}$, temos também $\displaystyle{ \int_{-1}^{1} e^{-x^2/2} dx \in\mathbb{R}}$. Logo, $\displaystyle{ \int_{-\infty}^{\infty} e^{-x^2/2} dx \in\mathbb{R}}$, digamos $c$. Segue então que
\begin{align*}
    c^2 = \int_{-\infty}^{\infty} e^{-x^2/2} dx\int_{-\infty}^{\infty} e^{-y^2/2} dy = \iint_{\mathbb{R}^2} e^{-\frac{x^2+y^2}{2}} dxdy = \int_{0}^{\infty}\int_{0}^{2\pi} re^{-r^2/2} drd\theta = 2\pi \int_{0}^{\infty} re^{-r^2/2} dr = 2\pi,
\end{align*}
ou seja, $c = \sqrt{2\pi}$, pois $c>0$. Logo, $\displaystyle{ \int_{\mathbb{R}} f(x) dx = \frac{1}{\sqrt{2\pi}} \int_{\mathbb{R}} g(x) dx = 1.}$ Note que $f$ é simétrica em torno de 0 e, além disso, a f.d. $F$ associada a $f$ é dada por
\begin{align*}
    F(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt, x\in\mathbb{R}.
\end{align*}
Verifica-se que $F$ não tem uma forma fechada, mas podemos aproximar seus valores numericamente. Como mencionado antes, temos $F(0) = 1/2 = 1 - F(0)$ e $F(x) = 1 - F(x), \forall x\in\mathbb{R}$. É comum também denotar $f$ por $\varphi$ e $F$ por $\Phi$ no caso da densidade e da f.d. de uma v.a. normal padrão, respectivamente.
\end{proof}
\begin{remark}
Tabelas de valores para a distribuição normal geralmente fornecem as probabilidades do tipo $P(0 < X < a)$. De fato, basta apenas esta probabilidade, pois $P(X\leq a) = 1/2 + P(0 < X < a), a>0$ e analogamente para os demais casos.
\end{remark}
De maneira geral, dizemos que $X$ é v.a. contínua com distribuição \textbf{normal de parâmetros} $\mu$ \textbf{e} $\sigma^2$, $X\sim N(\mu, \sigma^2)$ (veremos o que esses parâmetros significam mais à frente), se tem densidade dada por
\begin{align*}
    f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, x\in\mathbb{R},
\end{align*}
com $\mu,\sigma\in\mathbb{R}$ e $\sigma>0$. A verificação de que $f$ de fato é densidade é análoga ao que fizemos acima, bastando apenas efetuar a mudança de variável $y = \displaystyle{\frac{x-\mu}{\sigma}}$. Note também que $f$ é simétrica em torno de $\mu$.
\begin{center}
    \textbf{GRÁFICO P.110}
\end{center}
As variáveis aleatórios com distribuição normal ocorrem frequentemente em aplicações práticas. A Lei de Maxwell da Física afirma que, sob condições adequadas, as componentes da velocidade de uma molécula de gás estarão aleatoriamente distribuídas seguindo uma distribuição normal $N(0,\sigma^2)$, onde $\sigma^2$ depende de certas quantidades físicas. Entretanto, na maioria das aplicações, as v.a.'s de interesse têm distribuições que é \textbf{aproximadamente} normal. Por exemplo, erros instrumentais em experimentos físicos e variabilidade biológica (e.g., altura e massa) foram verificados, empiricamente, como possuindo distribuições aproximadamente normais. Veremos esse tipo de comportamente mais adiante.
\end{example}

\begin{example}[Gama, $X\sim\Gamma(\alpha, \lambda)$]
Dizemos que $X$ é v.a. contínua com distribuição \textbf{gama} de parâmetros $\alpha$ e $\lambda$ (positivos) se tem densidade dada por
\begin{align*}
    f(x) = \begin{cases}
    \frac{\lambda^{\alpha}}{\Gamma(\alpha)}x^{\alpha - 1}e^{-\lambda x}, x > 0 \\
    0, x\leq 0
    \end{cases}
\end{align*}
sendo
\begin{align*}
    \Gamma(\alpha) = \int_{0}^{\infty} x^{\alpha - 1}e^{-x} dx, \alpha > 0
\end{align*}
a \textbf{função gama}. Em particular, pode-se mostrar que $\Gamma(\alpha) \in\mathbb{R}^*_+$ para todo $\alpha$ real positivo, apesar de não haver uma forma fechada para a integral. Verificamos que $f$ é densidade.
\begin{proof}
É imediato que $f(x)\geq 0, \forall x\in\mathbb{R}$. Ademais, temos
\begin{align*}
    \int_{0}^{\infty} x^{\alpha - 1}e^{-\lambda x} dx = \int_{0}^{\infty} y^{\alpha - 1}\lambda^{1 - \alpha}e^{-y}\frac{1}{\lambda} dy = \frac{1}{\lambda^{\alpha}}\int_{0}^{\infty} y^{\alpha - 1}e^{-y} dy = \frac{\Gamma(\alpha)}{\lambda^{\alpha}},
\end{align*}
logo
\begin{align*}
    \int_{\mathbb{R}} f(x) dx = \int_{0}^{\infty} \frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1}e^{-\lambda x} dx = \frac{\lambda^{\alpha}}{\Gamma(\alpha)}\frac{\Gamma(\alpha)}{\lambda^{\alpha}} = 1.
\end{align*}
\end{proof}
A função gama possui algumas propriedades, muito úteis nos cálculos:
\begin{enumerate}[(i)]
    \item $\displaystyle{ \Gamma(1) = \int_{0}^{\infty} e^{-x} dx = 1 }$
    \item $\displaystyle{ \Gamma(1/2) = \int_{0}^{\infty} \frac{e^{-x}}{\sqrt{x}} dx = \sqrt{2} \int_{0}^{\infty} e^{-t^2/2} dt = \sqrt{2}\frac{1}{2}\sqrt{2\pi} = \sqrt{\pi}}$
    \item $\displaystyle{ \Gamma(\alpha + 1) = \int_{0}^{\infty} x^{\alpha} e^{-x} dx = -x^{\alpha}e^{-x}\Big|_{0}^{\infty} + \int_{0}^{\infty} \alpha x^{\alpha - 1} e^{-x} dx = \alpha\Gamma(\alpha)}$
    \item $\Gamma(1) = 1$ e $\Gamma(\alpha + 1) = \alpha\Gamma(\alpha)$ implicam $\Gamma(n) = (n-1)!, \forall n\in\mathbb{N}$
    \item $\Gamma(\alpha + 1) = \alpha\Gamma(\alpha)$ e $\Gamma(1/2) = \sqrt{\pi}$ implicam
    \begin{align*}
        \Gamma(n/2) = \frac{ \sqrt{\pi} }{ 2^{\frac{n-1}{2}} } \prod_{j=1}^{\frac{n-1}{2}} (n-2j) = \frac{ \sqrt{\pi} }{ 2^{\frac{n-1}{2}} }\frac{1}{2^{\frac{n-1}{2}}}\frac{(n-1)!}{\left( \frac{n-1}{2} \right)!} = \frac{ \sqrt{\pi} }{ 2^{n-1} }\frac{(n-1)!}{\left( \frac{n-1}{2} \right)!}, \forall n\in\mathbb{N} \text{ ímpar.}
    \end{align*}
    \begin{center}
        \textbf{GRÁFICO P.111}
    \end{center}
    \item $X\sim\text{Exp}(\lambda)\iff X\sim\Gamma(1, \lambda)$, ou seja, a distribuição exponencial é um caso particular da distribuição gama
    \item se $\alpha = m \in\mathbb{N}$ com $m\geq 2$, então a f.d. $F$ de $X\sim\Gamma(m,\lambda)$ tem uma forma fechada:
    \begin{align*}
        \int_{0}^x \frac{ \lambda^m y^{m-1} e^{-\lambda y} }{(m-1)!} dy &= \frac{ -(\lambda y)^{m-1} e^{-\lambda y} }{(m-1)!}\Big|_{0}^{x} + \int_{0}^x \frac{ \lambda^{m-1} y^{m-2} e^{-\lambda y} }{ (m-2)! } dy \\
        &= \int_{0}^x \frac{ \lambda^{m-1} y^{m-2} e^{-\lambda y} }{ (m-2)! } dy - \frac{ (\lambda x)^{m-1} e^{-\lambda x} }{(m-1)!} \\
        &\vdots \\
        &= 1 - \sum_{k=0}^{m-1} \frac{ (\lambda x)^k }{k!}e^{-\lambda x}, x>0,
    \end{align*}
    em que integramos por partes $m$ vezes. Isso sugere uma conexão com $Y\sim\text{Poisson}(\lambda x)$. De fato, segue do fato acima que $P(X\leq x) = P(Y\geq m)$, e essa conexão tem aplicações em processos de Poisson.
\end{enumerate}
\end{example}

\begin{remark}[Construção de funções de densidade]
Seja $g:\mathbb{R}\to\mathbb{R}$ uma função tal que $g(x)\geq 0, \forall x\in\mathbb{R}$ e $\displaystyle{ \int_{\mathbb{R}} g(x) dx = c\in\mathbb{R}^{*}_{+} }$. Logo, se $f(x) := g(x)/c, \forall x\in\mathbb{R}$, então $f$ é densidade.
\begin{example}
Seja $g(x) = \displaystyle{ \frac{1}{1+x^2}, x\in\mathbb{R} }$. Temos $g(x)\geq 0, \forall x\in\mathbb{R}$ e 
\begin{align*}
    \int_{\mathbb{R}} g(x) dx = \lim_{a\to +\infty} \left[ \arctan x \Big|_{-a}^{a} \right] = \pi\in\mathbb{R}^*_+.
\end{align*}
Logo, $f(x) = \displaystyle{ \frac{1}{\pi(1 + x^2)} }$ é densidade.
\end{example}
\end{remark}

\begin{example}[Cauchy padrão, $X\sim\text{Cauchy}(0,1)$]
Dizemos que $X$ é v.a. contínua com distribuição \textbf{Cauchy} de parâmetros 0 e 1 se tem densidade dada por
\begin{align*}
    f(x) = \frac{1}{\pi(1+x^2)}, \forall x\in\mathbb{R}.
\end{align*}
A f.d. $F$ associada a $f$ é dada por
\begin{align*}
    F(x) = \int_{-\infty}^x f(t) dt = \frac{1}{\pi}\arctan t\Big|_{-\infty}^{x} = \frac{1}{2} + \frac{1}{\pi}\arctan x, \forall x\in\mathbb{R}.
\end{align*}
Podemos considerar também $X\sim\text{Cauchy}(\mu, \beta)$, com $\mu$ real e $\beta$ real positivo, chamada \textbf{distribuição Cauchy} de parâmetros $\mu$ e $\beta$. Essa v.a. tem densidade dada por
\begin{align*}
    f(x) = \frac{\beta}{\pi(\beta^2 + (x-\mu)^2)}, \forall x\in\mathbb{R}.
\end{align*}
Verificamos que $f$ é densidade.
\begin{proof}
Note que $f(x) \geq 0, \forall x\in\mathbb{R}$ e que
\begin{align*}
    \int_{\mathbb{R}} f(x) dx = \frac{1}{\pi}\int_{-\infty}^{\infty} \frac{1}{\beta}\cdot\frac{ 1 }{ 1 + \left( \frac{x-\mu}{\beta} \right)^2 } dx = \frac{1}{\pi}\int_{-\infty}^{\infty} \frac{1}{1+y^2} dy = 1.
\end{align*}
\end{proof}
\end{example}

\subsection{Funções de variáveis aleatórias contínuas}
Sendo $X$ v.a. contínua e $g$ função em $\mathbb{R}$, estamos interessados em determinar a densidade $f_Y$ de $Y = g(X)$.

\begin{example}
Seja $Y = X^2$, i.e., $Y = g(X)$ com $g:\mathbb{R}\to\mathbb{R}$ tal que $g(x) = x^2$. Note que se $y\leq 0$, então $F_Y(y) = P(Y\leq y) = 0$ e, se $y>0$, então $F_Y(y) = P(Y\leq y) = P(X^2\leq y) = P(-\sqrt{y} \leq X\leq \sqrt{y}) = F_X(\sqrt{y}) - F_X(-\sqrt{y})$. Logo, segue que para $y>0$ temos
\begin{align*}
    f_Y(y) = F_X'(\sqrt{y})\frac{d}{dy}(\sqrt{y}) - F_X'(-\sqrt{y})\frac{d}{dy}(-\sqrt{y}) = \frac{1}{2\sqrt{y}}[f_X(\sqrt{y}) + f_X(-\sqrt{y})],
\end{align*}
ou seja, 
\begin{align*}
    f_Y(y) = \begin{cases}
    \frac{1}{2\sqrt{y}}[f_X(\sqrt{y}) + f_X(-\sqrt{y})], y > 0 \\
    0, y\leq 0
    \end{cases}.
\end{align*}
Essa fórmula vale para todo $y$ onde $F_X'(\sqrt{y})$ existe.
\end{example}

\begin{remark}
Se $X$ é v.a. contínua com densidade $f_X$ e $g$ é uma função discreta, i.e., com imagem finita ou enumerável, então $Y=g(X)$ é v.a. discreta.
\end{remark}

\begin{example}
Se $X\sim\text{Exp}(1)$ e $Y = I_{X\leq 3}$, i.e.,
\begin{align*}
    Y = g(X) = \begin{cases}
    1, X\leq 3 \\
    0, X > 3
    \end{cases},
\end{align*}
então os valores possíveis de $Y$ são 0 e 1. Temos $P(Y=1) = P(X\leq 3) = 1 - e^{-3}$, $P(Y=0) = P(X>3) = e^{-3}$ e $P(Y=y) = 0, \forall y\in\mathbb{R}\setminus\{0,1\}$. Logo,
\begin{align*}
    p_Y(y) = \begin{cases}
    e^{-3}, y = 0 \\
    1 - e^{-3}, y = 1 \\
    0, \text{ c.c.}
    \end{cases}
\end{align*}
é a f.p. de $Y$ (discreta!).
\end{example}

\begin{theorem}[Mudança de variável]
Seja $X$ v.a. contínua com f.d. $f_X$ tal que $f_X(x) = 0$ para todo $x\in I$, sendo $I$ um aberto da reta. Suponha que $g:\mathbb{R}\to\mathbb{R}$ seja uma função estritamente monótona e derivável em $I$. Então $Y=g(X)$ tem densidade dada por
\begin{align*}
    f_Y(y) = \begin{cases}
    f_x(g^{-1}(y))\left| \frac{d}{dy}g^{-1}(y) \right|, y\in g(I) \\
    0, y\notin g(I)
    \end{cases}.
\end{align*}
Podemos simplificar a notação tomando $x = g^{-1}(y)$.
\end{theorem}

\begin{proof}
Note que $g$ é derivável e inversível em $I$. Suponhamos $g$ estritamente crescente em $I$. Então $g^{-1}$ é estritamente crescente em $g(I)$. Daí, dado $y\in g(I)$, temos
\begin{align*}
    F_Y(y) = P(Y\leq y) = P(g(X)\leq y) = P(X\leq g^{-1}(y)) = F_X(g^{-1}(y)).
\end{align*}
Logo,
\begin{align*}
    f_Y(y) = \frac{d}{dy}\left[ F_X(g^{-1}(y)) \right] = f_X(g^{-1}(y))\underbrace{\frac{d}{dy}(g^{-1}(y))}_{>0},
\end{align*}
ou seja, temos
\begin{align*}
    f_Y(y) = f_x(g^{-1}(y))\left| \frac{d}{dy}g^{-1}(y)\right|.
\end{align*}
Se $g$ é estritamente decrescente em $I$, então $g^{-1}$ também o é e, dado $y\in g(I)$, temos
\begin{align*}
    F_Y(y) = P(Y\leq y) = P(g(X)\leq y) = P(X\geq g^{-1}(y)) = 1 - F_X(g^{-1}(y)).
\end{align*}
Logo,
\begin{align*}
    f_Y(y) = \frac{d}{dy}\left[1 - F_X(g^{-1}(y)) \right] = f_X(g^{-1}(y))\underbrace{\frac{d}{dy}(-g^{-1}(y))}_{>0},
\end{align*}
ou seja, temos
\begin{align*}
    f_Y(y) = f_x(g^{-1}(y))\left| \frac{d}{dy}g^{-1}(y)\right|.
\end{align*}
\end{proof}

\begin{example}
Sejam $X\sim\text{Exp}(\lambda)$ e $Y = X^{1/\beta}, \beta\in\mathbb{R}^*$. Temos
\begin{align*}
    f_X(x) = \begin{cases}
    \lambda e^{-\lambda x}, x > 0 \\
    0, x\leq 0
    \end{cases}.
\end{align*}
Ademais, $Y = g(X)$ com $g$ função da reta dada por $g(x) = x^{1/\beta}$. Note que $g$ é crescente e derivável em $(0, +\infty)$ e também
\begin{align*}
    y = x^{1/\beta} \iff x = y^{\beta} \implies \frac{dx}{dy} = \beta y^{\beta - 1}.
\end{align*}
Daí, se $y>0$, temos
\begin{align*}
    f_Y(y) = f_X(x)\left|\frac{dx}{dy}\right| = \lambda e^{-\lambda y^{\beta}}|\beta y^{\beta -1}|
\end{align*}
e, portanto,
\begin{align*}
    f_Y(y) = \begin{cases}
    \lambda |\beta| y^{\beta - 1}e^{-\lambda y^{\beta}}, y> 0 \\
    0, y\leq 0.
    \end{cases}
\end{align*}
\end{example}

\begin{example}
Sejam $X$ v.a. contínua com densidade $f_X$ e $a,b\in\mathbb{R}$ com $b\neq 0$. Se $Y = a + bX$, então
\begin{align*}
    y = a + bx \iff x = \frac{y-a}{b} \implies \frac{dx}{dy} = \frac{1}{b},
\end{align*}
com $g$ função da reta dada por $g(x) = a + bx$ estritamente monótona em $\mathbb{R}$ (que é um aberto). Logo, dadao $y\in\mathbb{R}$, temos
\begin{align*}
    f_Y(y) = f_X\left( \frac{y-a}{b} \right)\left|\frac{1}{b}\right|.
\end{align*}
\end{example}

\begin{proposition}
Sejam $X$ v.a. contínua e $\mu, \sigma\in\mathbb{R}$ com $\sigma > 0$. Temos
\begin{enumerate}[(i)]
    \item se $Y = \displaystyle{ \frac{X-\mu}{\sigma} }$, então $X\sim N(\mu, \sigma^2) \iff Y\sim N(0,1)$
    \item se $X\sim N(\mu, \sigma^2)$ e $Y = a + bX$ com $a,b\in\mathbb{R}$ e $b\neq 0$, então $Y\sim N(a+b\mu, b^2\sigma^2)$
    \item se $X\sim N(0,1)$ e $Y=-X$, então $Y\sim N(0,1)$
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[(i)]
    \item Note que $g:\mathbb{R}\to\mathbb{R}$ dada por $g(x) = \displaystyle{ \frac{x-\mu}{\sigma} }$ é estritamente crescente e derivável na reta. Daí, temos
    \begin{align*}
        x = \sigma y + \mu \implies \frac{dx}{dy} = \sigma.
    \end{align*}
    Logo, 
    \begin{align*}
        f_Y(y) = \frac{1}{\sqrt{2\pi}\sigma}e^{ -\frac{(y-\mu)^2}{2\sigma^2} }\sigma = \frac{1}{\sqrt{2\pi}}e^{-y^2/2}, y\in\mathbb{R},
    \end{align*}
    ou seja, $Y\sim N(0,1)$. Reciprocamente, se $Y\sim N(0,1)$ então
    \begin{align*}
        f_X(x) = f_Y(g(x))\left|\frac{dy}{dx}\right| = \frac{1}{\sqrt{2\pi}\sigma}\exp[-\frac{1}{2}(\frac{x-\mu}{\sigma})^2]
    \end{align*}
    ou seja, $X\sim N(\mu, \sigma^2)$.
    \item Utilizando o exemplo anterior, temos
    \begin{align*}
        f_Y(y) = f_X(g^{-1}(y))\left|\frac{dx}{dy}\right| = \frac{1}{\sqrt{2\pi}\sigma}\exp[-\frac{1}{2}\frac{1}{\sigma^2}(\frac{y-a}{b} - \mu)^2]\left|\frac{1}{b}\right| = \frac{1}{\sqrt{2\pi}\sigma|b|} \exp[ -\frac{1}{2}\frac{ [ y - (a+b\mu) ]^2 }{ (\sigma b)^2 } ],
    \end{align*}
    ou seja, $Y\sim N(a + b\mu, b^2\sigma^2)$.
    \item Note que $g:\mathbb{R}\to\mathbb{R}$ dada por $g(x) = -x$ é estritamente decrescente e derivável em $\mathbb{R}$. Ademais, temos $x = -y$ e $dx/dy = -1$, de modo que
    \begin{align*}
        f_Y(y) = f_X(g^{-1}(y)) \left|\frac{dx}{dy}\right| = \frac{1}{\sqrt{2\pi}}\exp[ -\frac{1}{2}(-y)^2 ]|-1| = \frac{1}{\sqrt{2\pi}} e^{-y^2/2},
    \end{align*}
    ou seja, $Y\sim N(0,1)$.
\end{enumerate}
\end{proof}

\section{Vetores aleatórios contínuos}
Começamos com as definições gerais, generalizando de maneira natural as definições vistas na seção anterior.

\subsection{Definições gerais}
\begin{definition}[Vetor aleatório]
Sejam $X_1, X_2, \dots, X_n$ v.a.'s absolutamente contínuas, definidas em $(\Omega, \mathcal{A}, P)$. Chamamos $\overline{X} = (X_1, \dots, X_n)$ de \textbf{vetor aleatório contínuo} ($n$-dimensional).

Dito de outro modo, $\overline{X}$ é vetor aleatório contínuo se sua função de distribuição $F_{X_1, \dots, X_n}$ é dada por
\begin{align*}
    F_{\overline{X}}(\overline{x}) = P(X_1\leq x_1, \dots, X_n\leq x_n) = \int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_n} f(u_1, \dots, u_n) du_n\cdots du_1, \forall \overline{x}\in\mathbb{R}^n,
\end{align*}
para alguma função $f:\mathbb{R}^n\to\mathbb{R}$ função de densidade $n$-dimensional, isto é, $f(\overline{x}) \geq 0 \forall\overline{x}\in\mathbb{R}$ e $\displaystyle{ \int_{\mathbb{R}} \cdots \int_{\mathbb{R}} f(x_1, \dots, x_n) dx_1\cdots dx_n = 1 }$. Nesse caso, dizemos que $f$ é \textbf{função de densidade} de $\overline{X}$ e denotamos $f:= f_{\overline{X}}$.
\end{definition}

\begin{definition}
Seja $F_{X_1, \dots, X_n}$ a f.d. de $(X_1, \dots, X_n)$. Para cada $k\in\{1,\dots,n\}$, definimos a \textbf{função de distribuição marginal} de $X_k$ por
\begin{align*}
    F_{X_k}(x_k) = P(X_k\leq x_k) = \lim_{x_i\to +\infty, i\neq k} F_{X_1, \dots, X_n}(x_1, \dots, x_n), x_k\in\mathbb{R}.
\end{align*}
Podemos definir a f.d. marginal de pares, triplas e $r$-uplas de maneira inteiramente análoga.
\end{definition}

\subsection{Distribuições marginais e independência}
\paragraph{Caso bidimensional.} Seja $(X,Y)$ um vetor aleatório contínuo com densidade conjunta $f = f_{X,Y}$ e f.d. conjunta $F = F_{X,Y}$. Temos
\begin{align*}
    F(x,y) = P(X\leq x, Y\leq y) = \int_{-\infty}^x\int_{-\infty}^y f(u,v) dvdu, \forall (x,y)\in\mathbb{R}^2
\end{align*}
e também
\begin{align*}
    P(a < X\leq b, c < Y\leq d) = \int_{a}^{b}\int_{c}^{d} f(x,y) dydx.
\end{align*}
De maneira geral, 
\begin{align*}
    P((X,Y)\in A) = \iint_A f(x,y) dxdy, \forall A\in\mathcal{B}^2(\mathbb{R}).
\end{align*}
Podemos, ainda, obter as densidades marginais de $X$ e $Y$. Note que
\begin{align*}
    F_X(x) = P(X\leq x) = P(X\leq x, Y\in\mathbb{R}) = \int_{-\infty}^{x}\int_{-\infty}^{\infty} f(u,y) dydu.
\end{align*}
Por outro lado, sabemos que
\begin{align*}
    F_X(x) = \int_{-\infty}^{x} f_X(u) du.
\end{align*}
Logo, como ambas as igualdades valem para todo $x\in\mathbb{R}$, devemos ter
\begin{align*}
    f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) dy, \forall x\in\mathbb{R}.
\end{align*}
De maneira análoga, temos
\begin{align*}
    F_Y(y) = \int_{-\infty}^{y}\int_{-\infty}^{\infty} f(x,v) dxdv
\end{align*}
e
\begin{align*}
    f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) dx, \forall y\in\mathbb{R}.
\end{align*}
Sob algumas fracas condições de regularidades e para $(x,y)$ ponto de continuidade de $f$, temos
\begin{align*}
    \frac{\partial}{\partial y}F(x,y) = \int_{-\infty}^x \left( \frac{\partial}{\partial y} \int_{-\infty}^y f(u,v) dv \right) du = \int_{-\infty}^x f(u,y) du,
\end{align*}
de modo que
\begin{align*}
    \frac{\partial^2}{\partial x\partial y}F(x,y) = f(x,y)
\end{align*}
e também, de maneira análoga,
\begin{align*}
    \frac{\partial^2}{\partial y\partial x}F(x,y) = f(x,y).
\end{align*}

\begin{example}
Sejam $X, Y$ v.a.'s contínuas com f.d. conjunta $F$ dada por
\begin{align*}
    F(x,y) = \begin{cases}
    0, x<0 \text{ e } y<0 \\
    \frac{x}{5}(1 - e^{-y}), 0\leq x\leq 5 \text{ e } y\geq 0 \\
    1 - e^{-y}, x\geq 5 \text{ e } y\geq 0
    \end{cases}.
\end{align*}
Note que
\begin{align*}
    F_X(x) = \lim_{y\to\infty} F(x,y) = \begin{cases}
    0, x < 0 \\
    x/5, 0\leq x < 5 \\
    1, x\geq 5
    \end{cases}
\end{align*}
e também
\begin{align*}
    F_Y(y) = \lim_{x\to\infty} F(x,y) = \begin{cases}
    0, y < 0 \\
    1 - e^{-y}, y\geq 0
    \end{cases}.
\end{align*}
Ademais, para todo $(x,y)\in\mathbb{R}^2$ com $x\notin\{0,5\}$ e $y\neq 0$, temos
\begin{align*}
    \frac{\partial}{\partial y}F(x,y) = \begin{cases}
    0, x < 0 \text{ e } y < 0 \\
    \frac{x}{5}e^{-y}, 0 < x < 5 \text{ e } y > 0 \\
    e^{-y}, x > 5 \text{ e } y>0
    \end{cases}
\end{align*}
e também
\begin{align*}
    \frac{\partial^2}{\partial x\partial y}F(x,y) = \begin{cases}
    0, x < 0 \text{ e } y < 0 \\
    \frac{e^{-y}}{5}, 0 < x < 5 \text{ e } y > 0 \\
    0, x > 5 \text{ e } y>0
    \end{cases}.
\end{align*}
Portanto, 
\begin{align*}
    f_{X,Y}(x,y) = \begin{cases}
    e^{-y}/5, 0 < x < 5 \text{ e } y > 0 \\
    0, \text{ c.c.}
    \end{cases}
\end{align*}
e obtemos
\begin{align*}
    f_X(x) &= \frac{d}{dx}F_X(x) = \begin{cases}
    1/5, 0 < x < 5 \\
    0, \text{ c.c.}
    \end{cases} \\
    f_Y(y) &= \frac{d}{dy}F_Y(y) = \begin{cases}
    e^{-y}, y > 0 \\
    0, \text{ c.c.}
    \end{cases},
\end{align*}
ou seja, $X\sim U(0,5)$ e $Y\sim\text{Exp}(1)$.
\end{example}

\begin{example}
a
\end{example}



\subsection{Função de distribuição $n$-dimensional}

\subsection{Funções de vetores aleatórios}

\subsection{Densidades condicionais}

\subsubsection{Regra de Bayes}

\section{Esperança de variáveis aleatórias contínuas}

\subsection{Definição}

\subsection{Esperança de função de variável aleatória contínua}

\subsection{Momentos de uma variável aleatória contínua}

\subsection{Esperança condicional}




\end{document}